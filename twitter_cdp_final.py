#!/usr/bin/env python3
"""
Twitter/X CDP æŠ“å–å·¥å…· - ç”¨æˆ·æ‰§è¡Œç‰ˆ
é€šè¿‡ Chrome DevTools Protocol æ§åˆ¶å·²ç™»å½•çš„æµè§ˆå™¨æŠ“å–æ¨æ–‡

ä½¿ç”¨æ–¹æ³•ï¼š
1. å¯åŠ¨ Chrome with remote debugging (è§ä¸‹æ–¹ START_CHROME è¯´æ˜)
2. åœ¨ Chrome ä¸­ç™»å½• Twitter/X
3. è®¿é—®ç›®æ ‡ç”¨æˆ·ä¸»é¡µ
4. è¿è¡Œæœ¬è„šæœ¬

ä½œè€…: 0xC1A
æ—¥æœŸ: 2026-02-04
"""

import json
import requests
import time
import sys
from datetime import datetime
from pathlib import Path

# ============ é…ç½® ============
CHROME_PORT = 9222
OUTPUT_DIR = Path('twitter_cdp_exports')
MAX_SCROLLS = 100  # æœ€å¤§æ»šåŠ¨æ¬¡æ•°
SCROLL_DELAY = 2   # æ¯æ¬¡æ»šåŠ¨åç­‰å¾…æ—¶é—´(ç§’)
# =============================

def check_chrome_connection():
    """æ£€æŸ¥ Chrome DevTools æ˜¯å¦å¯ç”¨"""
    try:
        resp = requests.get(f'http://localhost:{CHROME_PORT}/json/version', timeout=5)
        if resp.status_code == 200:
            version = resp.json()
            return True, version.get('Browser', 'unknown')
    except:
        pass
    return False, None

def get_twitter_page():
    """è·å– Twitter/X é¡µé¢çš„ WebSocket URL"""
    try:
        resp = requests.get(f'http://localhost:{CHROME_PORT}/json/list', timeout=10)
        pages = resp.json()
        
        for p in pages:
            url = p.get('url', '')
            # æŸ¥æ‰¾ x.com åŸŸåï¼Œæ’é™¤ devtools é¡µé¢
            if ('x.com' in url or 'twitter.com' in url) and 'devtools' not in url:
                return {
                    'id': p['id'],
                    'url': url,
                    'ws_url': p['webSocketDebuggerUrl'],
                    'title': p.get('title', 'Unknown')
                }
    except Exception as e:
        print(f"âŒ è·å–é¡µé¢åˆ—è¡¨å¤±è´¥: {e}")
    return None

def eval_js(ws_url, js_code, timeout=30):
    """é€šè¿‡ WebSocket æ‰§è¡Œ JavaScript"""
    try:
        import websocket
        ws = websocket.create_connection(ws_url, timeout=timeout)
        
        ws.send(json.dumps({
            'id': 1,
            'method': 'Runtime.evaluate',
            'params': {
                'expression': js_code,
                'returnByValue': True,
                'awaitPromise': True
            }
        }))
        
        result = ws.recv()
        ws.close()
        
        data = json.loads(result)
        if 'result' in data and 'result' in data['result']:
            return data['result']['result'].get('value')
    except Exception as e:
        print(f"  âš ï¸ JS æ‰§è¡Œå‡ºé”™: {e}")
    return None

def expand_collapsed_tweets(ws_url):
    """ç‚¹å‡»æ‰€æœ‰ "Show more" æŒ‰é’®å±•å¼€æŠ˜å çš„æ¨æ–‡"""
    js_code = """
    (function() {
        // æŸ¥æ‰¾æ‰€æœ‰ "Show more" æŒ‰é’®
        const buttons = document.querySelectorAll('button[role="button"]');
        let clicked = 0;
        
        buttons.forEach(btn => {
            const text = btn.innerText || btn.textContent || '';
            // åŒ¹é… "Show more" æˆ–ä¸­æ–‡ "æ˜¾ç¤ºæ›´å¤š"
            if (text.match(/show more|æ˜¾ç¤ºæ›´å¤š/i)) {
                btn.click();
                clicked++;
            }
        });
        
        // ä¹Ÿå°è¯•é€šè¿‡ aria-label æŸ¥æ‰¾
        const altButtons = document.querySelectorAll('[aria-label*="Show more"], [aria-label*="æ˜¾ç¤ºæ›´å¤š"]');
        altButtons.forEach(btn => {
            if (!btn.clicked) {
                btn.click();
                clicked++;
            }
        });
        
        return 'Clicked ' + clicked + ' "Show more" buttons';
    })()
    """
    result = eval_js(ws_url, js_code)
    if result:
        print(f"      {result}")
    time.sleep(1)  # ç­‰å¾…å±•å¼€åŠ¨ç”»

def extract_tweets_from_page(ws_url):
    """ä»å½“å‰é¡µé¢æå–æ‰€æœ‰æ¨æ–‡æ•°æ®"""
    
    # å…ˆå±•å¼€æ‰€æœ‰æŠ˜å çš„æ¨æ–‡
    expand_collapsed_tweets(ws_url)
    
    js_code = """
    (function() {
        const tweets = [];
        const articles = document.querySelectorAll('article[data-testid="tweet"]');
        
        articles.forEach(article => {
            try {
                const tweet = {};
                
                // æ¨æ–‡ ID å’Œ URL
                const statusLink = article.querySelector('a[href*="/status/"]');
                if (statusLink) {
                    const href = statusLink.getAttribute('href');
                    const match = href.match(/\\/status\\/(\\d+)/);
                    if (match) {
                        tweet.id = match[1];
                        tweet.url = 'https://x.com' + href;
                    }
                }
                
                if (!tweet.id) return; // è·³è¿‡æ— æ•ˆæ¡ç›®
                
                // ä½œè€…ä¿¡æ¯
                const userNameDiv = article.querySelector('div[data-testid="User-Name"]');
                if (userNameDiv) {
                    const userLink = userNameDiv.querySelector('a');
                    if (userLink) {
                        const href = userLink.getAttribute('href');
                        if (href) tweet.author = href.split('/')[1];
                    }
                    // æ˜¾ç¤ºåç§°
                    const nameSpan = userNameDiv.querySelector('span span');
                    if (nameSpan) tweet.author_name = nameSpan.innerText;
                }
                
                // æ¨æ–‡å†…å®¹ - å°è¯•å¤šç§æ–¹å¼è·å–å®Œæ•´æ–‡æœ¬
                let textContent = '';
                
                // æ–¹æ³•1: é€šè¿‡ tweetText æ•°æ®å±æ€§
                const textDiv = article.querySelector('[data-testid="tweetText"]');
                if (textDiv) {
                    // è·å–æ‰€æœ‰æ–‡æœ¬èŠ‚ç‚¹ï¼ŒåŒ…æ‹¬è¢«æŠ˜å çš„éƒ¨åˆ†
                    textContent = textDiv.innerText || textDiv.textContent || '';
                    
                    // å¦‚æœå†…å®¹è¢«æˆªæ–­ï¼Œå°è¯•è·å–å®Œæ•´å†…å®¹
                    // Twitter æœ‰æ—¶ä¼šå°†å®Œæ•´æ–‡æœ¬æ”¾åœ¨ aria-label ä¸­
                    if (textContent.length < 100 && textDiv.getAttribute('aria-label')) {
                        const ariaText = textDiv.getAttribute('aria-label');
                        if (ariaText.length > textContent.length) {
                            textContent = ariaText;
                        }
                    }
                }
                
                // æ–¹æ³•2: å°è¯•è·å–æ‰€æœ‰ span ä¸­çš„æ–‡æœ¬ï¼ˆæœ‰æ—¶æ¨æ–‡åˆ†æ•£åœ¨å¤šä¸ª span ä¸­ï¼‰
                if (!textContent) {
                    const spans = article.querySelectorAll('span');
                    let combinedText = '';
                    spans.forEach(span => {
                        const txt = span.innerText || span.textContent;
                        if (txt && txt.length > 10 && !txt.includes('@')) {
                            combinedText += txt + ' ';
                        }
                    });
                    if (combinedText.length > 50) {
                        textContent = combinedText.trim();
                    }
                }
                
                tweet.text = textContent;
                
                // å‘å¸ƒæ—¶é—´
                const timeElem = article.querySelector('time');
                tweet.created_at = timeElem ? timeElem.getAttribute('datetime') : '';
                
                // äº’åŠ¨æ•°æ® (å›å¤/è½¬å‘/ç‚¹èµ)
                const actions = ['reply', 'retweet', 'like'];
                actions.forEach(action => {
                    const btn = article.querySelector(`[data-testid="${action}"]`);
                    if (btn) {
                        const ariaLabel = btn.getAttribute('aria-label') || '';
                        // æå–æ•°å­—ï¼Œå¤„ç† "5,231 likes" æ ¼å¼
                        const match = ariaLabel.replace(/,/g, '').match(/(\\d+)/);
                        tweet[action + '_count'] = match ? parseInt(match[1]) : 0;
                    } else {
                        tweet[action + '_count'] = 0;
                    }
                });
                
                // æ˜¯å¦å›å¤åˆ«äººçš„æ¨æ–‡
                const replyContext = article.querySelector('[data-testid="socialContext"]');
                tweet.is_reply = !!replyContext;
                if (replyContext) {
                    tweet.reply_to_text = replyContext.innerText;
                }
                
                // åª’ä½“æ–‡ä»¶
                const photos = article.querySelectorAll('[data-testid="tweetPhoto"]');
                const videos = article.querySelectorAll('[data-testid="tweetVideo"]');
                tweet.media_count = photos.length + videos.length;
                tweet.has_media = tweet.media_count > 0;
                
                // å¼•ç”¨æ¨æ–‡ (Quote Tweet)
                const quoted = article.querySelector('[data-testid="quotedTweet"]');
                if (quoted) {
                    const quotedText = quoted.querySelector('[data-testid="tweetText"]');
                    const quotedAuthor = quoted.querySelector('div[data-testid="User-Name"] span');
                    tweet.quoted_tweet = {
                        'text': quotedText ? quotedText.innerText.substring(0, 200) : '',
                        'author': quotedAuthor ? quotedAuthor.innerText : ''
                    };
                }
                
                tweets.push(tweet);
            } catch(e) {
                // å¿½ç•¥å•ä¸ªæ¨æ–‡æå–é”™è¯¯
            }
        });
        
        return {
            'count': tweets.length,
            'tweets': tweets
        };
    })()
    """
    
    result = eval_js(ws_url, js_code)
    if result and isinstance(result, dict):
        return result.get('tweets', [])
    return []

def scroll_page_down(ws_url, times=1):
    """å‘ä¸‹æ»šåŠ¨é¡µé¢"""
    for i in range(times):
        eval_js(ws_url, """
            window.scrollTo({
                top: document.body.scrollHeight,
                behavior: 'smooth'
            });
        """)
        time.sleep(SCROLL_DELAY)

def scrape_tweets(username, max_scrolls=MAX_SCROLLS):
    """
    ä¸»æŠ“å–å‡½æ•°
    
    Args:
        username: Twitter ç”¨æˆ·å (ä¸å« @)
        max_scrolls: æœ€å¤§æ»šåŠ¨æ¬¡æ•°
    """
    print("=" * 70)
    print(f"ğŸ¦ Twitter CDP æŠ“å–å·¥å…·")
    print(f"   ç›®æ ‡ç”¨æˆ·: @{username}")
    print("=" * 70)
    
    # æ­¥éª¤ 1: æ£€æŸ¥ Chrome è¿æ¥
    print("\nğŸ“¡ æ­¥éª¤ 1: æ£€æŸ¥ Chrome DevTools è¿æ¥...")
    connected, browser_version = check_chrome_connection()
    
    if not connected:
        print("âŒ æ— æ³•è¿æ¥åˆ° Chrome")
        print("\nè¯·æŒ‰ä»¥ä¸‹æ­¥éª¤æ“ä½œ:\n")
        print("1ï¸âƒ£  å…³é—­æ‰€æœ‰ Chrome çª—å£")
        print("2ï¸âƒ£  åœ¨ç»ˆç«¯è¿è¡Œä»¥ä¸‹å‘½ä»¤å¯åŠ¨ Chrome:\n")
        print(f"""
/Applications/Google\\ Chrome.app/Contents/MacOS/Google\\ Chrome \\
    --remote-debugging-port={CHROME_PORT} \\
    --remote-allow-origins='*' \\
    --user-data-dir=/tmp/chrome_dev_profile
""")
        print("3ï¸âƒ£  åœ¨ Chrome ä¸­ç™»å½• Twitter/X")
        print(f"4ï¸âƒ£  è®¿é—® https://x.com/{username}")
        print("5ï¸âƒ£  é‡æ–°è¿è¡Œæœ¬è„šæœ¬\n")
        return []
    
    print(f"âœ… å·²è¿æ¥åˆ° Chrome ({browser_version})")
    
    # æ­¥éª¤ 2: æŸ¥æ‰¾ Twitter é¡µé¢
    print("\nğŸ“„ æ­¥éª¤ 2: æŸ¥æ‰¾ Twitter é¡µé¢...")
    page_info = get_twitter_page()
    
    if not page_info:
        print("âŒ æœªæ‰¾åˆ° Twitter/X é¡µé¢")
        print(f"\nè¯·åœ¨ Chrome ä¸­è®¿é—®: https://x.com/{username}")
        return []
    
    print(f"âœ… æ‰¾åˆ°é¡µé¢: {page_info['title']}")
    print(f"   URL: {page_info['url'][:60]}...")
    
    ws_url = page_info['ws_url']
    
    # æ­¥éª¤ 3: å¼€å§‹æŠ“å–
    print(f"\nğŸ” æ­¥éª¤ 3: å¼€å§‹æŠ“å–æ¨æ–‡...")
    print(f"   æœ€å¤§æ»šåŠ¨æ¬¡æ•°: {max_scrolls}")
    print(f"   æ¯æ¬¡æ»šåŠ¨ç­‰å¾…: {SCROLL_DELAY}ç§’")
    print()
    
    all_tweets = {}  # ç”¨å­—å…¸å»é‡
    no_new_count = 0  # è¿ç»­æ— æ–°æ•°æ®çš„æ¬¡æ•°
    
    for scroll_num in range(max_scrolls):
        # æå–å½“å‰é¡µé¢çš„æ¨æ–‡
        tweets = extract_tweets_from_page(ws_url)
        
        new_count = 0
        for t in tweets:
            tid = t.get('id')
            if tid and tid not in all_tweets:
                all_tweets[tid] = t
                new_count += 1
        
        total = len(all_tweets)
        
        # æ¯ 5 æ¬¡æ»šåŠ¨æˆ–å‘ç°æ–°æ•°æ®æ—¶æ˜¾ç¤ºè¿›åº¦
        if scroll_num % 5 == 0 or new_count > 0:
            print(f"   æ»šåŠ¨ {scroll_num:3d}/{max_scrolls}: +{new_count:2d} æ¡æ–°æ¨æ–‡ | æ€»è®¡: {total} æ¡")
        
        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤šå†…å®¹
        if new_count == 0:
            no_new_count += 1
            if no_new_count >= 3:  # è¿ç»­ 3 æ¬¡æ— æ–°æ•°æ®
                print(f"\nâœ… æ²¡æœ‰æ›´å¤šæ¨æ–‡äº†ï¼Œåœæ­¢æŠ“å–")
                break
        else:
            no_new_count = 0
        
        # å‘ä¸‹æ»šåŠ¨
        scroll_page_down(ws_url, times=1)
    
    return list(all_tweets.values())

def save_results(username, tweets):
    """ä¿å­˜æŠ“å–ç»“æœåˆ°å¤šç§æ ¼å¼"""
    if not tweets:
        print("âŒ æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
        return None
    
    OUTPUT_DIR.mkdir(exist_ok=True)
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # æŒ‰æ—¶é—´æ’åº
    tweets.sort(key=lambda x: x.get('created_at', ''), reverse=True)
    
    # ç»Ÿè®¡æ•°æ®
    stats = {
        'total': len(tweets),
        'original': sum(1 for t in tweets if not t.get('is_reply')),
        'replies': sum(1 for t in tweets if t.get('is_reply')),
        'with_media': sum(1 for t in tweets if t.get('has_media')),
        'total_likes': sum(t.get('like_count', 0) for t in tweets)
    }
    
    print(f"\nğŸ“Š ç»Ÿè®¡ç»“æœ:")
    print(f"   æ€»è®¡æ¨æ–‡: {stats['total']} æ¡")
    print(f"   åŸåˆ›æ¨æ–‡: {stats['original']} æ¡")
    print(f"   å›å¤: {stats['replies']} æ¡")
    print(f"   å¸¦åª’ä½“: {stats['with_media']} æ¡")
    print(f"   æ€»ç‚¹èµ: {stats['total_likes']}")
    
    # 1. ä¿å­˜å®Œæ•´ JSON
    json_file = OUTPUT_DIR / f'{username}_cdp_{timestamp}.json'
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump({
            'username': username,
            'scraped_at': datetime.now().isoformat(),
            'stats': stats,
            'tweets': tweets
        }, f, ensure_ascii=False, indent=2)
    
    # 2. ä¿å­˜ Markdown
    md_file = OUTPUT_DIR / f'{username}_cdp_{timestamp}.md'
    with open(md_file, 'w', encoding='utf-8') as f:
        f.write(f"# @{username} çš„æ¨æ–‡å­˜æ¡£\n\n")
        f.write(f"æŠ“å–æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"æ¨æ–‡æ•°é‡: {stats['total']} æ¡\n\n")
        f.write(f"- åŸåˆ›: {stats['original']} æ¡\n")
        f.write(f"- å›å¤: {stats['replies']} æ¡\n")
        f.write(f"- å¸¦åª’ä½“: {stats['with_media']} æ¡\n\n")
        f.write("---\n\n")
        
        for i, t in enumerate(tweets, 1):
            date = t.get('created_at', '')[:10] if t.get('created_at') else 'æœªçŸ¥'
            text = t.get('text', '').strip()
            url = t.get('url', '')
            author = t.get('author', username)
            
            # æ ‡è®°
            is_reply = t.get('is_reply', False)
            has_media = t.get('has_media', False)
            mark = 'ğŸ’¬' if is_reply else 'ğŸ“'
            media_mark = 'ğŸ“' if has_media else ''
            
            f.write(f"### {i}. {mark} {media_mark} [{date}]({url})\n\n")
            
            # æ¨æ–‡å†…å®¹
            for line in text.split('\n'):
                f.write(f"> {line}\n")
            
            f.write(f"\n")
            f.write(f"ğŸ‘¤ @{author}  ")
            f.write(f"ğŸ‘ {t.get('like_count', 0)}  ")
            f.write(f"ğŸ’¬ {t.get('reply_count', 0)}  ")
            f.write(f"ğŸ”„ {t.get('retweet_count', 0)}\n\n")
            
            # å¼•ç”¨æ¨æ–‡
            if t.get('quoted_tweet'):
                qt = t['quoted_tweet']
                f.write(f"> ğŸ’¬ å¼•ç”¨ @{qt.get('author', 'unknown')}: {qt.get('text', '')[:100]}...\n\n")
            
            f.write("---\n\n")
    
    # 3. ä¿å­˜ CSV
    csv_file = OUTPUT_DIR / f'{username}_cdp_{timestamp}.csv'
    import csv
    with open(csv_file, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(['åºå·', 'æ—¥æœŸ', 'ä½œè€…', 'å†…å®¹', 'URL', 'æ˜¯å¦å›å¤', 'ç‚¹èµ', 'å›å¤', 'è½¬å‘', 'åª’ä½“'])
        for i, t in enumerate(tweets, 1):
            writer.writerow([
                i,
                t.get('created_at', ''),
                t.get('author', ''),
                t.get('text', '').replace('\n', ' '),
                t.get('url', ''),
                'æ˜¯' if t.get('is_reply') else 'å¦',
                t.get('like_count', 0),
                t.get('reply_count', 0),
                t.get('retweet_count', 0),
                'æ˜¯' if t.get('has_media') else 'å¦'
            ])
    
    print(f"\nâœ… æ–‡ä»¶å·²ä¿å­˜:")
    print(f"   ğŸ“„ JSON: {json_file}")
    print(f"   ğŸ“ Markdown: {md_file}")
    print(f"   ğŸ“Š CSV: {csv_file}")
    
    return json_file

def main():
    """ä¸»å‡½æ•°"""
    # è·å–ç”¨æˆ·å
    if len(sys.argv) > 1:
        username = sys.argv[1].lstrip('@')
    else:
        username = input("è¯·è¾“å…¥ Twitter ç”¨æˆ·å (ä¸å« @): ").strip().lstrip('@')
    
    if not username:
        print("âŒ ç”¨æˆ·åä¸èƒ½ä¸ºç©º")
        return
    
    # æ£€æŸ¥ä¾èµ–
    try:
        import websocket
    except ImportError:
        print("ğŸ“¦ å®‰è£…ä¾èµ–: websocket-client...")
        import subprocess
        subprocess.run([sys.executable, '-m', 'pip', 'install', 'websocket-client', '-q'])
        print("âœ… ä¾èµ–å®‰è£…å®Œæˆï¼Œè¯·é‡æ–°è¿è¡Œè„šæœ¬\n")
        return
    
    # æ‰§è¡ŒæŠ“å–
    tweets = scrape_tweets(username)
    
    if tweets:
        save_results(username, tweets)
        
        # æ˜¾ç¤ºæœ€æ–° 5 æ¡
        print(f"\nğŸ“ æœ€æ–° 5 æ¡æ¨æ–‡:")
        for t in tweets[:5]:
            date = t.get('created_at', '')[:10] if t.get('created_at') else 'æœªçŸ¥'
            text = t.get('text', '')[:60].replace('\n', ' ')
            mark = 'ğŸ’¬' if t.get('is_reply') else 'ğŸ“'
            print(f"   {mark} [{date}] {text}...")
        
        print(f"\nğŸ‰ å®Œæˆ! æ•°æ®ä¿å­˜åœ¨: {OUTPUT_DIR}/")
    else:
        print("\nâŒ æœªèƒ½æŠ“å–åˆ°æ¨æ–‡")
        print("\nå¯èƒ½çš„åŸå› :")
        print("   1. Chrome æœªå¼€å¯ remote debugging")
        print("   2. æœªåœ¨ Chrome ä¸­ç™»å½• Twitter")
        print("   3. ç›®æ ‡ç”¨æˆ·ä¸å­˜åœ¨æˆ–æ¨æ–‡å—ä¿æŠ¤")

if __name__ == '__main__':
    main()
