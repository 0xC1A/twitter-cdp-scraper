#!/usr/bin/env python3
"""
CDP Spider - é€šç”¨ç½‘é¡µæŠ“å–æ¡†æ¶
åŸºäº Chrome DevTools Protocol çš„çµæ´»æ•°æ®æå–å·¥å…·

ç‰¹ç‚¹ï¼š
- é€šè¿‡é…ç½®æ–‡ä»¶å®šä¹‰æŠ“å–é€»è¾‘
- æ”¯æŒæ»šåŠ¨åŠ è½½ã€åˆ†é¡µã€ç‚¹å‡»å±•å¼€
- æ™ºèƒ½æ»šåŠ¨ç­–ç•¥åº”å¯¹è™šæ‹Ÿæ»šåŠ¨ï¼ˆå¦‚ Twitter/Xï¼‰
- å¤šç§æ•°æ®å¯¼å‡ºæ ¼å¼
- å†…ç½®å¸¸è§ç½‘ç«™é¢„è®¾é…ç½®

ä½œè€…: 0xC1A
"""

import json
import requests
import time
import re
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional, Callable
from dataclasses import dataclass, field


@dataclass
class ExtractorConfig:
    """æ•°æ®æå–é…ç½®"""
    name: str                          # æå–å™¨åç§°
    url_pattern: str                   # URL åŒ¹é…æ¨¡å¼

    # é€‰æ‹©å™¨é…ç½®
    item_selector: str                 # åˆ—è¡¨é¡¹é€‰æ‹©å™¨ (å¦‚: 'article[data-testid="tweet"]')
    field_selectors: Dict[str, str]    # å­—æ®µé€‰æ‹©å™¨ {å­—æ®µå: CSSé€‰æ‹©å™¨}

    # æ»šåŠ¨/åˆ†é¡µé…ç½®
    scroll_enabled: bool = True        # æ˜¯å¦å¯ç”¨æ»šåŠ¨
    scroll_times: int = 0              # æœ€å¤§æ»šåŠ¨æ¬¡æ•° (0è¡¨ç¤ºä¸é™)
    scroll_delay: float = 2.0          # æ»šåŠ¨é—´éš”(ç§’)
    scroll_selector: Optional[str] = None  # æ»šåŠ¨å®¹å™¨é€‰æ‹©å™¨ (Noneåˆ™æ»šåŠ¨æ•´ä¸ªé¡µé¢)

    # å±•å¼€é…ç½®
    expand_selectors: List[str] = field(default_factory=list)  # éœ€è¦ç‚¹å‡»å±•å¼€çš„å…ƒç´ 
    expand_delay: float = 1.0          # å±•å¼€åç­‰å¾…æ—¶é—´

    # åª’ä½“ä¸‹è½½é…ç½®
    download_media: bool = False       # æ˜¯å¦ä¸‹è½½åª’ä½“æ–‡ä»¶

    # æ•°æ®å¤„ç†
    field_processors: Dict[str, Callable] = field(default_factory=dict)  # å­—æ®µåå¤„ç†å™¨
    item_filter: Optional[Callable] = None  # é¡¹ç›®è¿‡æ»¤å‡½æ•°

    # å¯¼å‡ºé…ç½®
    id_field: str = 'id'               # å”¯ä¸€æ ‡è¯†å­—æ®µ
    sort_field: str = ''               # æ’åºå­—æ®µ
    sort_reverse: bool = True          # å€’åºæ’åº


class CDPSpider:
    """CDP æŠ“å–æ¡†æ¶ä¸»ç±»"""

    def __init__(self, chrome_port: int = 9222, output_dir: str = 'spider_exports'):
        self.chrome_port = chrome_port
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)

    def _check_chrome(self) -> tuple[bool, str]:
        """æ£€æŸ¥ Chrome DevTools è¿æ¥"""
        try:
            resp = requests.get(f'http://localhost:{self.chrome_port}/json/version', timeout=5)
            if resp.status_code == 200:
                return True, resp.json().get('Browser', 'unknown')
        except:
            pass
        return False, ''

    def _get_page(self, url_pattern: str) -> Optional[Dict]:
        """è·å–åŒ¹é…çš„é¡µé¢"""
        try:
            resp = requests.get(f'http://localhost:{self.chrome_port}/json/list', timeout=10)
            pages = resp.json()

            for p in pages:
                page_url = p.get('url', '')
                if re.search(url_pattern, page_url) and 'devtools' not in page_url:
                    return {
                        'id': p['id'],
                        'url': page_url,
                        'ws_url': p['webSocketDebuggerUrl'],
                        'title': p.get('title', 'Unknown')
                    }
        except Exception as e:
            print(f"âŒ è·å–é¡µé¢åˆ—è¡¨å¤±è´¥: {e}")
        return None

    def _eval_js(self, ws_url: str, js_code: str, timeout: int = 30) -> Any:
        """æ‰§è¡Œ JavaScript"""
        try:
            import websocket
            ws = websocket.create_connection(ws_url, timeout=timeout)
            ws.send(json.dumps({
                'id': 1,
                'method': 'Runtime.evaluate',
                'params': {
                    'expression': js_code,
                    'returnByValue': True,
                    'awaitPromise': True
                }
            }))
            result = ws.recv()
            ws.close()

            data = json.loads(result)
            if 'result' in data and 'result' in data['result']:
                return data['result']['result'].get('value')
        except Exception as e:
            print(f"  âš ï¸ JS æ‰§è¡Œé”™è¯¯: {e}")
        return None

    def _expand_items(self, ws_url: str, config: ExtractorConfig):
        """ç‚¹å‡»å±•å¼€æ‰€æœ‰æŠ˜å é¡¹ - ä»…å±•å¼€ä¸»æ¨æ–‡çš„é•¿æ–‡æœ¬ï¼Œé¿å…ç‚¹å‡»å¼•ç”¨æ¨æ–‡å¯¼è‡´è·³è½¬"""
        for selector in config.expand_selectors:
            for attempt in range(5):  # å¢åŠ å°è¯•æ¬¡æ•°
                js_code = f"""
                (function() {{
                    // æ£€æŸ¥æ˜¯å¦åœ¨æ—¶é—´çº¿é¡µé¢
                    if (window.location.pathname.includes('/status/')) {{
                        return {{status: 'wrong_page', msg: 'åœ¨æ¨æ–‡è¯¦æƒ…é¡µ'}};
                    }}

                    let clicked = 0;

                    // æ–¹æ³•1: é€šè¿‡ data-testid æŸ¥æ‰¾
                    const items1 = document.querySelectorAll('[data-testid="tweet-text-show-more-link"]');

                    // æ–¹æ³•2: é€šè¿‡æ–‡æœ¬å†…å®¹æŸ¥æ‰¾æ‰€æœ‰åŒ…å« "Show more" çš„ span/button
                    // ä¸»æ¨æ–‡çš„ show more é€šå¸¸æ˜¯ tweetText åŒºåŸŸå†…çš„ span
                    const allArticles = document.querySelectorAll('article[data-testid="tweet"]');

                    // ä¼˜å…ˆä½¿ç”¨æ–¹æ³•1
                    items1.forEach(item => {{
                        if (!item || item.offsetParent === null || item.getAttribute('data-expanded')) {{
                            return;
                        }}

                        // æ£€æŸ¥æ˜¯å¦åœ¨ä¸»æ¨æ–‡å†…ï¼ˆä¸æ˜¯å¼•ç”¨æ¨æ–‡ï¼‰
                        // å¼•ç”¨æ¨æ–‡é€šå¸¸åœ¨ä¸€ä¸ªåµŒå¥—çš„ article æˆ–ç‰¹å®šå®¹å™¨å†…
                        const isQuoteTweet = item.closest('div[role="link"]') !== null ||
                                            item.closest('[data-testid="quotedTweet"]') !== null ||
                                            item.closest('article') !== item.closest('article[data-testid="tweet"]');

                        if (isQuoteTweet) {{
                            return;
                        }}

                        item.setAttribute('data-expanded', 'true');
                        item.click();
                        clicked++;
                    }});

                    // å¦‚æœæ–¹æ³•1æ²¡ç‚¹åˆ°ï¼Œå°è¯•æ–¹æ³•2ï¼šåœ¨æ¯ä¸ª article å†…æŸ¥æ‰¾ show more
                    if (clicked === 0) {{
                        allArticles.forEach(article => {{
                            // åªå¤„ç†ä¸»æ¨æ–‡çš„ tweetText åŒºåŸŸ
                            const tweetText = article.querySelector('[data-testid="tweetText"]');
                            if (!tweetText) return;

                            // åœ¨ tweetText å†…æŸ¥æ‰¾ show more æŒ‰é’®
                            // å®ƒå¯èƒ½æ˜¯ä¸€ä¸ª span æˆ– buttonï¼ŒåŒ…å« "Show more" æ–‡æœ¬
                            const allElements = tweetText.querySelectorAll('span, button');

                            allElements.forEach(el => {{
                                if (el.getAttribute('data-expanded')) return;

                                const text = (el.innerText || el.textContent || '').trim();
                                const ariaLabel = (el.getAttribute('aria-label') || '').trim();

                                // åŒ¹é… Show moreï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
                                if (text.toLowerCase() === 'show more' ||
                                    ariaLabel.toLowerCase() === 'show more' ||
                                    text.toLowerCase().includes('show more')) {{

                                    el.setAttribute('data-expanded', 'true');
                                    el.click();
                                    clicked++;
                                }}
                            }});
                        }});
                    }}

                    return {{status: 'success', clicked: clicked}};
                }})()
                """
                result = self._eval_js(ws_url, js_code)

                if isinstance(result, dict):
                    if result.get('status') == 'wrong_page':
                        print(f"      âš ï¸ æ£€æµ‹åˆ°åœ¨æ¨æ–‡è¯¦æƒ…é¡µï¼Œåœæ­¢å±•å¼€")
                        return
                    clicked = result.get('clicked', 0)
                else:
                    clicked = int(result) if isinstance(result, (int, float)) else 0

                if clicked > 0:
                    print(f"      å±•å¼€ {clicked} ä¸ªä¸»æ¨æ–‡æŠ˜å é¡¹ (å°è¯• {attempt + 1})")
                    time.sleep(config.expand_delay)
                else:
                    break

    def _scroll_page(self, ws_url: str, config: ExtractorConfig, step: int = 1) -> dict:
        """
        æ»šåŠ¨é¡µé¢ - ä½¿ç”¨å°æ­¥é•¿æ»šåŠ¨é¿å…è™šæ‹Ÿæ»šåŠ¨å¯¼è‡´çš„æ•°æ®ä¸¢å¤±
        è¿”å›è¯¦ç»†çš„æ»šåŠ¨ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ˜¯å¦çœŸæ­£æ»šåŠ¨äº†ï¼ˆç”¨äºæ£€æµ‹åº•éƒ¨ï¼‰

        Args:
            step: æ»šåŠ¨æ­¥æ•°ï¼Œæ¯æ¬¡æ»šåŠ¨ä¸€å±çš„ä¸€éƒ¨åˆ†

        Returns:
            {
                'scrolled': è¯·æ±‚çš„æ»šåŠ¨è·ç¦»,
                'actualScrolled': å®é™…æ»šåŠ¨è·ç¦»,
                'viewportHeight': è§†å£é«˜åº¦,
                'newPosition': æ–°æ»šåŠ¨ä½ç½®,
                'pageHeight': é¡µé¢æ€»é«˜åº¦,
                'hitBottom': æ˜¯å¦ç¢°åˆ°åº•éƒ¨ï¼ˆå®é™…æ»šåŠ¨ < è¯·æ±‚æ»šåŠ¨çš„50%ï¼‰,
                'scrollPercent': æ»šåŠ¨ç™¾åˆ†æ¯”
            }
        """
        js_code = """
        (function() {
            const viewportHeight = window.innerHeight;
            const scrollDistance = Math.floor(viewportHeight * 0.7);
            const beforeScroll = window.pageYOffset || document.documentElement.scrollTop;
            const pageHeight = document.body.scrollHeight;
            const maxScroll = pageHeight - viewportHeight;

            window.scrollTo({
                top: beforeScroll + scrollDistance,
                behavior: 'smooth'
            });

            // ç­‰å¾…æ»šåŠ¨åŠ¨ç”»å¼€å§‹
            return new Promise((resolve) => {
                setTimeout(() => {
                    const afterScroll = window.pageYOffset || document.documentElement.scrollTop;
                    const actualScrolled = afterScroll - beforeScroll;
                    const scrollPercent = maxScroll > 0 ? (afterScroll / maxScroll * 100).toFixed(1) : 100;

                    // å¦‚æœå®é™…æ»šåŠ¨è·ç¦»å°äºè¯·æ±‚è·ç¦»çš„50%ï¼Œè®¤ä¸ºç¢°åˆ°äº†åº•éƒ¨
                    const hitBottom = actualScrolled < scrollDistance * 0.5 || afterScroll >= maxScroll - 10;

                    resolve({
                        scrolled: scrollDistance,
                        actualScrolled: actualScrolled,
                        viewportHeight: viewportHeight,
                        newPosition: afterScroll,
                        pageHeight: pageHeight,
                        hitBottom: hitBottom,
                        scrollPercent: parseFloat(scrollPercent)
                    });
                }, 300); // ç»™æ»šåŠ¨åŠ¨ç”»ä¸€ç‚¹æ—¶é—´
            });
        })()
        """

        result = self._eval_js(ws_url, js_code)
        time.sleep(config.scroll_delay)
        return result or {}

    def _get_scroll_info(self, ws_url: str) -> dict:
        """è·å–å½“å‰æ»šåŠ¨ä¿¡æ¯"""
        js_code = """
        (function() {
            const scrollTop = window.pageYOffset || document.documentElement.scrollTop;
            const scrollHeight = document.body.scrollHeight;
            const viewportHeight = window.innerHeight;
            const maxScroll = Math.max(1, scrollHeight - viewportHeight);

            return {
                scrollTop: scrollTop,
                scrollHeight: scrollHeight,
                viewportHeight: viewportHeight,
                scrollPercent: ((scrollTop / maxScroll) * 100).toFixed(1)
            };
        })()
        """
        return self._eval_js(ws_url, js_code) or {}

    def _get_top_visible_item_id(self, ws_url: str, config: ExtractorConfig) -> Optional[str]:
        """
        è·å–è§†å£ä¸­æœ€é¡¶éƒ¨å¯è§çš„æ¨æ–‡ID
        ç”¨äºæ£€æµ‹æ˜¯å¦çœŸçš„åœ¨å‘å‰æ»šåŠ¨
        """
        js_code = f"""
        (function() {{
            const articles = document.querySelectorAll('{config.item_selector}');
            const viewportTop = window.pageYOffset || document.documentElement.scrollTop;
            const viewportHeight = window.innerHeight;

            for (const article of articles) {{
                const rect = article.getBoundingClientRect();
                const articleTop = rect.top + viewportTop;
                const idEl = article.querySelector('{config.id_field}') ||
                            article.querySelector('a[href*="/status/"]');

                // æ‰¾åˆ°ç¬¬ä¸€ä¸ªåœ¨è§†å£å†…æˆ–åˆšå¥½åœ¨è§†å£ä¸Šæ–¹çš„æ¨æ–‡
                if (articleTop >= viewportTop - 100 &&
                    articleTop <= viewportTop + viewportHeight * 0.5) {{
                    let id = '';
                    if (idEl) {{
                        id = idEl.getAttribute('href') || idEl.innerText || '';
                    }}
                    if (!id) {{
                        // å¤‡ç”¨ï¼šä½¿ç”¨ç´¢å¼•
                        id = 'idx_' + Array.from(articles).indexOf(article);
                    }}
                    return {{
                        id: id.trim(),
                        position: articleTop
                    }};
                }}
            }}
            return null;
        }})()
        """
        return self._eval_js(ws_url, js_code)

    def _get_all_visible_item_ids(self, ws_url: str, config: ExtractorConfig) -> List[str]:
        """
        è·å–å½“å‰DOMä¸­æ‰€æœ‰å¯è§çš„æ¨æ–‡IDåˆ—è¡¨
        ç”¨äºåˆ¤æ–­æ˜¯å¦æ‰€æœ‰å¯è§æ¨æ–‡éƒ½å·²è¢«æŠ“å–

        Returns:
            å¯è§æ¨æ–‡IDåˆ—è¡¨ï¼ˆæŒ‰åœ¨DOMä¸­çš„é¡ºåºï¼‰
        """
        js_code = f"""
        (function() {{
            const articles = document.querySelectorAll('{config.item_selector}');
            const viewportTop = window.pageYOffset || document.documentElement.scrollTop;
            const viewportHeight = window.innerHeight;
            const ids = [];

            articles.forEach((article, index) => {{
                const rect = article.getBoundingClientRect();
                const articleTop = rect.top + viewportTop;
                const articleBottom = articleTop + rect.height;

                // æ£€æŸ¥æ¨æ–‡æ˜¯å¦åœ¨è§†å£å†…ï¼ˆæˆ–éƒ¨åˆ†å¯è§ï¼‰
                const isVisible = (articleTop < viewportTop + viewportHeight + 100) &&
                                  (articleBottom > viewportTop - 100);

                if (isVisible) {{
                    // å°è¯•è·å–ID
                    let id = '';
                    const idEl = article.querySelector('{config.id_field}') ||
                                article.querySelector('a[href*="/status/"]');
                    if (idEl) {{
                        id = idEl.getAttribute('href') || idEl.innerText || '';
                    }}
                    if (!id) {{
                        id = 'idx_' + index;
                    }}
                    ids.push(id.trim());
                }}
            }});

            return ids;
        }})()
        """
        result = self._eval_js(ws_url, js_code)
        return result if isinstance(result, list) else []

    def _check_all_visible_items_crawled(self, ws_url: str, config: ExtractorConfig,
                                          crawled_ids: set) -> dict:
        """
        æ£€æŸ¥å½“å‰DOMä¸­æ‰€æœ‰å¯è§æ¨æ–‡æ˜¯å¦éƒ½å·²è¢«æŠ“å–

        Args:
            crawled_ids: å·²æŠ“å–çš„æ¨æ–‡IDé›†åˆ

        Returns:
            {
                'all_crawled': bool,  # æ‰€æœ‰å¯è§æ¨æ–‡æ˜¯å¦éƒ½å·²æŠ“å–
                'visible_count': int,  # å¯è§æ¨æ–‡æ•°é‡
                'crawled_count': int,  # å·²æŠ“å–çš„å¯è§æ¨æ–‡æ•°é‡
                'uncrawled_ids': list  # æœªæŠ“å–çš„å¯è§æ¨æ–‡ID
            }
        """
        visible_ids = self._get_all_visible_item_ids(ws_url, config)

        if not visible_ids:
            return {
                'all_crawled': False,
                'visible_count': 0,
                'crawled_count': 0,
                'uncrawled_ids': []
            }

        # å¤„ç†IDæ ¼å¼ï¼ˆæå–æ¨æ–‡IDï¼‰
        def extract_id(id_str: str) -> str:
            if '/status/' in id_str:
                match = re.search(r'/status/(\d+)', id_str)
                if match:
                    return match.group(1)
            return id_str

        visible_ids_clean = {extract_id(vid) for vid in visible_ids}
        crawled_ids_clean = {extract_id(cid) for cid in crawled_ids}

        uncrawled = visible_ids_clean - crawled_ids_clean

        return {
            'all_crawled': len(uncrawled) == 0,
            'visible_count': len(visible_ids_clean),
            'crawled_count': len(visible_ids_clean) - len(uncrawled),
            'uncrawled_ids': list(uncrawled)
        }

    def _extract_items(self, ws_url: str, config: ExtractorConfig, download_media: bool = False, media_dir: Path = None) -> List[Dict]:
        """æå–å½“å‰é¡µé¢çš„æ‰€æœ‰é¡¹ç›®"""
        # å…ˆå±•å¼€æŠ˜å é¡¹
        if config.expand_selectors:
            self._expand_items(ws_url, config)

        # æ„å»ºæå– JS
        field_extractors = []
        for field_name, selector in config.field_selectors.items():
            # è·³è¿‡åª’ä½“å­—æ®µï¼Œæˆ‘ä»¬å•ç‹¬å¤„ç†
            if field_name in ['image_urls', 'video_urls']:
                continue

            # åˆ¤æ–­æ˜¯å¦éœ€è¦ä¼˜å…ˆè·å– hrefï¼ˆå¦‚ id å­—æ®µæˆ–é€‰æ‹©å™¨åŒ…å«é“¾æ¥ç›¸å…³ï¼‰
            prefer_href = field_name in ['id', 'url', 'link'] or 'href' in selector
            
            # å¯¹ time å­—æ®µç‰¹æ®Šå¤„ç†ï¼šä¼˜å…ˆè·å– datetime å±æ€§
            is_time_field = field_name == 'time' or selector == 'time'

            field_extractors.append(f"""
                // {field_name}
                try {{
                    const {field_name}El = article.querySelector('{selector}');
                    if ({field_name}El) {{
                        let text = '';
                        
                        // å¯¹äº time å…ƒç´ ï¼Œä¼˜å…ˆè·å– datetime å±æ€§ï¼ˆç²¾ç¡®æ—¶é—´ï¼‰
                        if ({str(is_time_field).lower()}) {{
                            text = {field_name}El.getAttribute('datetime') || 
                                   {field_name}El.getAttribute('title') || 
                                   {field_name}El.innerText || 
                                   {field_name}El.textContent || '';
                        }} else if ({str(prefer_href).lower()}) {{
                            // å¯¹äº id/url/link å­—æ®µï¼Œä¼˜å…ˆè·å– href
                            text = {field_name}El.getAttribute('href') || '';
                            if (!text) {{
                                text = {field_name}El.innerText || {field_name}El.textContent || '';
                            }}
                        }} else {{
                            // å…¶ä»–å­—æ®µä¼˜å…ˆä½¿ç”¨ innerText
                            text = {field_name}El.innerText || {field_name}El.textContent || '';
                            if (!text) {{
                                text = {field_name}El.getAttribute('href') || '';
                            }}
                        }}

                        // ä¹Ÿå°è¯• aria-label
                        if (!text) {{
                            text = {field_name}El.getAttribute('aria-label') || '';
                        }}

                        item['{field_name}'] = text.trim();
                    }}
                }} catch(e) {{}}
            """)

        # æ·»åŠ åª’ä½“æå–ä»£ç 
        media_extractor = """
            // æå–å›¾ç‰‡ URL
            try {
                const images = article.querySelectorAll('[data-testid="tweetPhoto"] img');
                const imageUrls = Array.from(images).map(img => img.src).filter(Boolean);
                if (imageUrls.length > 0) {
                    item['image_urls'] = imageUrls.join(',');
                    item['image_count'] = imageUrls.length;
                }
            } catch(e) {}

            // æå–è§†é¢‘æ ‡è®°
            try {
                const video = article.querySelector('[data-testid="videoPlayer"], [data-testid="videoComponent"]');
                if (video) {
                    item['has_video'] = true;
                }
            } catch(e) {}
        """

        js_code = f"""
        (function() {{
            const items = [];
            const articles = document.querySelectorAll('{config.item_selector}');

            articles.forEach((article, index) => {{
                try {{
                    const item = {{_index: index}};
                    {''.join(field_extractors)}
                    {media_extractor}
                    items.push(item);
                }} catch(e) {{}}
            }});

            return items;
        }})()
        """

        result = self._eval_js(ws_url, js_code)
        items = result if isinstance(result, list) else []

        # å¦‚æœå¯ç”¨äº†åª’ä½“ä¸‹è½½ï¼ŒåŒæ—¶ä¸‹è½½å›¾ç‰‡
        if download_media and media_dir:
            for item in items:
                image_urls = item.get('image_urls', '')
                if image_urls:
                    urls = [u.strip() for u in image_urls.split(',') if u.strip()]
                    downloaded = []

                    for url in urls:
                        tweet_id = str(item.get('id', 'unknown'))[:20]
                        filename = f"{tweet_id}_{url.split('/')[-1].split('?')[0]}"
                        if '.' not in filename:
                            filename += '.jpg'

                        save_path = media_dir / filename

                        if self._download_via_chrome(ws_url, url, save_path):
                            downloaded.append(filename)

                    if downloaded:
                        item['downloaded_images'] = ','.join(downloaded)

        return items

    def crawl(self, config: ExtractorConfig) -> List[Dict]:
        """
        æ‰§è¡ŒæŠ“å– - ä½¿ç”¨æ™ºèƒ½æ»šåŠ¨ç­–ç•¥åº”å¯¹è™šæ‹Ÿæ»šåŠ¨

        Args:
            config: æå–å™¨é…ç½®

        Returns:
            æŠ“å–çš„æ•°æ®åˆ—è¡¨
        """
        print("=" * 70)
        print(f"ğŸ•·ï¸  CDP Spider - {config.name}")
        print("=" * 70)

        # æ£€æŸ¥ Chrome
        print("\nğŸ“¡ è¿æ¥ Chrome...")
        connected, browser = self._check_chrome()
        if not connected:
            print("âŒ æ— æ³•è¿æ¥åˆ° Chrome")
            print(f"\nè¯·å…ˆå¯åŠ¨ Chrome:")
            print(f"  /Applications/Google\\ Chrome.app/Contents/MacOS/Google\\ Chrome \\")
            print(f"      --remote-debugging-port={self.chrome_port} \\")
            print(f"      --remote-allow-origins='*' \\")
            print(f"      --user-data-dir=/tmp/chrome_dev_profile")
            return []
        print(f"âœ… å·²è¿æ¥ ({browser})")

        # æŸ¥æ‰¾ç›®æ ‡é¡µé¢
        print(f"\nğŸ“„ æŸ¥æ‰¾é¡µé¢: {config.url_pattern}")
        page = self._get_page(config.url_pattern)
        if not page:
            print("âŒ æœªæ‰¾åˆ°åŒ¹é…çš„é¡µé¢")
            print("è¯·åœ¨ Chrome ä¸­æ‰“å¼€ç›®æ ‡é¡µé¢")
            return []
        print(f"âœ… æ‰¾åˆ°é¡µé¢: {page['title'][:50]}")

        # å¼€å§‹æŠ“å–
        print(f"\nğŸ” å¼€å§‹æŠ“å–...")
        scroll_limit_str = f"{config.scroll_times}" if config.scroll_times > 0 else "ä¸é™"
        print(f"   æ»šåŠ¨ç­–ç•¥: å°æ­¥é•¿æ»šåŠ¨ + å³æ—¶æå–ï¼ˆåº”å¯¹è™šæ‹Ÿæ»šåŠ¨ï¼‰")
        print(f"   æœ€å¤§æ»šåŠ¨æ¬¡æ•°: {scroll_limit_str}")

        all_items = {}
        ws_url = page['ws_url']
        no_new_count = 0  # è¿ç»­æ²¡æœ‰æ–°æ•°æ®çš„æ¬¡æ•°
        prev_scroll_top = 0
        prev_scroll_height = 0  # ä¸Šä¸€æ¬¡çš„é¡µé¢é«˜åº¦
        min_scroll_rounds = 10  # æœ€å°‘æ»šåŠ¨æ¬¡æ•°ï¼ˆé˜²æ­¢é•¿æ¨æ–‡è¯¯åˆ¤ï¼‰
        last_top_item_id = None  # ä¸Šä¸€æ¬¡è§†å£é¡¶éƒ¨çš„æ¨æ–‡ID
        stuck_count = 0  # è§†å£é¡¶éƒ¨æ¨æ–‡æœªå˜åŒ–çš„æ¬¡æ•°
        
        # ç¡®è®¤æ¨¡å¼ï¼šæ£€æµ‹åˆ°ç»“æŸä¿¡å·åï¼Œç»§ç»­æ»šåŠ¨ confirm_rounds æ¬¡ç¡®è®¤
        confirm_mode = False  # æ˜¯å¦è¿›å…¥ç¡®è®¤æ¨¡å¼
        confirm_rounds = 10   # ç¡®è®¤æ¨¡å¼éœ€è¦æ»šåŠ¨çš„æ¬¡æ•°
        confirm_remaining = 0 # ç¡®è®¤æ¨¡å¼å‰©ä½™æ¬¡æ•°
        confirm_trigger_reason = "" # è§¦å‘ç¡®è®¤æ¨¡å¼çš„åŸå› 
        
        # åª’ä½“ä¸‹è½½é…ç½®
        download_media = getattr(config, 'download_media', False)
        media_dir = None
        if download_media:
            media_dir = self.output_dir / f"media_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            media_dir.mkdir(exist_ok=True)
            print(f"   åª’ä½“ä¸‹è½½: å¯ç”¨ -> {media_dir}")

        # é‡ç½®æ»šåŠ¨ä½ç½®åˆ°é¡¶éƒ¨ï¼ˆç¡®ä¿ä»ç¬¬ä¸€æ¡æ¨æ–‡å¼€å§‹æŠ“å–ï¼‰
        print("\nğŸ“ é‡ç½®æ»šåŠ¨ä½ç½®åˆ°é¡¶éƒ¨...")
        self._eval_js(ws_url, "window.scrollTo({top: 0, behavior: 'instant'});")
        time.sleep(1)  # ç­‰å¾…è™šæ‹Ÿæ»šåŠ¨é‡æ–°æ¸²æŸ“
        print("   âœ… å·²å›åˆ°é¡¶éƒ¨ï¼Œå¼€å§‹æŠ“å–\n")

        # ç¡®å®šæœ€å¤§æ»šåŠ¨æ¬¡æ•°ï¼ˆ0è¡¨ç¤ºä¸é™ï¼Œä½¿ç”¨ä¸€ä¸ªå¾ˆå¤§çš„æ•°ï¼‰
        max_scroll_times = config.scroll_times if config.scroll_times > 0 else 10000
        
        for i in range(max_scroll_times if config.scroll_enabled else 1):
            # æå–æ•°æ®ï¼ˆåœ¨æ»šåŠ¨å‰ä¹Ÿæå–ä¸€æ¬¡ï¼Œç¡®ä¿ç¬¬ä¸€å±çš„æ•°æ®ï¼‰
            items = self._extract_items(ws_url, config, download_media, media_dir)

            new_count = 0
            duplicate_count = 0
            for item in items:
                item_id = item.get(config.id_field) or item.get('_index')
                if item_id:
                    if item_id not in all_items:
                        # åº”ç”¨å­—æ®µå¤„ç†å™¨
                        for field, processor in config.field_processors.items():
                            if field in item:
                                item[field] = processor(item[field])

                        # åº”ç”¨è¿‡æ»¤å™¨
                        if config.item_filter is None or config.item_filter(item):
                            all_items[item_id] = item
                            new_count += 1
                    else:
                        duplicate_count += 1

            # è·å–æ»šåŠ¨ä¿¡æ¯
            scroll_info = self._get_scroll_info(ws_url)
            scroll_percent = float(scroll_info.get('scrollPercent', 0))
            current_scroll_top = scroll_info.get('scrollTop', 0)
            current_scroll_height = scroll_info.get('scrollHeight', 0)

            # æ£€æµ‹é¡µé¢é«˜åº¦æ˜¯å¦å¢é•¿ï¼ˆæœ‰æ–°å†…å®¹åŠ è½½ï¼‰
            height_grew = current_scroll_height > prev_scroll_height

            # è·å–è§†å£é¡¶éƒ¨æ¨æ–‡IDï¼Œæ£€æµ‹æ˜¯å¦å¡ä½
            top_item = self._get_top_visible_item_id(ws_url, config)
            top_item_id = top_item.get('id') if top_item else None

            if top_item_id == last_top_item_id:
                stuck_count += 1
            else:
                stuck_count = 0
                last_top_item_id = top_item_id

            # æ£€æŸ¥å½“å‰DOMä¸­æ‰€æœ‰å¯è§æ¨æ–‡æ˜¯å¦éƒ½å·²è¢«æŠ“å–
            crawled_ids_set = set(all_items.keys())
            visible_check = self._check_all_visible_items_crawled(ws_url, config, crawled_ids_set)
            all_visible_crawled = visible_check.get('all_crawled', False)
            visible_uncrawled = visible_check.get('uncrawled_ids', [])

            # æ˜¾ç¤ºè¿›åº¦
            progress_bar = self._make_progress_bar(scroll_percent)
            all_duplicates = len(items) > 0 and new_count == 0  # å½“å‰è·å–çš„æ‰€æœ‰æ¨æ–‡éƒ½æ˜¯é‡å¤çš„
            status_marker = "â†‘" if height_grew else ("âœ“" if all_duplicates else " ")
            height_indicator = f"H+{current_scroll_height - prev_scroll_height:,}" if height_grew else ""
            stuck_indicator = f" (stuck:{stuck_count})" if stuck_count > 0 else ""
            visible_indicator = f" V:{visible_check['crawled_count']}/{visible_check['visible_count']}" if visible_check['visible_count'] > 0 else ""
            print(f"   ç¬¬ {i+1:2d} è½® | {progress_bar} | "
                  f"+{new_count:3d} æ–°æ•°æ® | "
                  f"é‡å¤:{duplicate_count:2d} | "
                  f"æ€»è®¡:{len(all_items):4d} æ¡ [{status_marker}] {height_indicator}{stuck_indicator}{visible_indicator}")

            # è°ƒè¯•ä¿¡æ¯ï¼šå¦‚æœæœ‰æœªæŠ“å–çš„å¯è§æ¨æ–‡
            if visible_uncrawled and i > 5:
                print(f"      âš ï¸ å‘ç° {len(visible_uncrawled)} æ¡å¯è§ä½†æœªæŠ“å–çš„æ¨æ–‡")

            # æ£€æŸ¥æ˜¯å¦éœ€è¦åœæ­¢
            if not config.scroll_enabled:
                break

            # åˆ¤æ–­æ¡ä»¶ï¼šå½“å‰è·å–çš„æ‰€æœ‰æ¨æ–‡éƒ½å·²ç»è¢«æŠ“å–è¿‡ï¼ˆä¸”ç¡®å®è·å–åˆ°äº†æ¨æ–‡ï¼‰
            if len(items) > 0 and new_count == 0:
                no_new_count += 1
            else:
                no_new_count = 0
                # æœ‰æ–°æ•°æ®æ—¶ï¼Œå¦‚æœä¹‹å‰åœ¨ç¡®è®¤æ¨¡å¼ï¼Œé€€å‡ºç¡®è®¤æ¨¡å¼
                if confirm_mode:
                    print(f"      ğŸ“¢ ç¡®è®¤æ¨¡å¼ä¸­æ–­ï¼šå‘ç° {new_count} æ¡æ–°æ•°æ®")
                    confirm_mode = False
                    confirm_remaining = 0

            # æ‰§è¡Œæ»šåŠ¨ï¼Œè·å–æ»šåŠ¨ç»“æœ
            scroll_result = self._scroll_page(ws_url, config, step=i+1)
            hit_bottom = scroll_result.get('hitBottom', False)
            actual_scrolled = scroll_result.get('actualScrolled', 0)

            # æ£€æŸ¥æ˜¯å¦æ»¡è¶³ç»“æŸä¿¡å·
            done_check = self._check_if_really_done(
                ws_url=ws_url,
                no_new_count=no_new_count,
                scroll_percent=scroll_percent,
                prev_scroll_top=prev_scroll_top,
                all_duplicates_in_round=all_duplicates,
                current_round=i+1,
                min_rounds=min_scroll_rounds,
                height_grew=height_grew,
                current_height=current_scroll_height,
                stuck_count=stuck_count,
                all_visible_crawled=all_visible_crawled,
                hit_bottom=hit_bottom,
                actual_scrolled=actual_scrolled,
                visible_count=visible_check.get('visible_count', 0)
            )

            # ç¡®è®¤æ¨¡å¼é€»è¾‘
            if done_check['done']:
                confidence = done_check.get('confidence', 'low')
                
                if confidence == 'high':
                    # å¼ºä¿¡å·ç›´æ¥ç»“æŸ
                    print(f"   âœ… {done_check['reason']}")
                    break
                elif not confirm_mode:
                    # ä¸­ç­‰/å¼±ä¿¡å·ï¼Œè¿›å…¥ç¡®è®¤æ¨¡å¼
                    confirm_mode = True
                    confirm_remaining = confirm_rounds
                    confirm_trigger_reason = done_check['reason']
                    print(f"   âš ï¸  {done_check['reason']}")
                    print(f"      è¿›å…¥ç¡®è®¤æ¨¡å¼ï¼šç»§ç»­æ»šåŠ¨ {confirm_rounds} æ¬¡ç¡®è®¤...")
                # å¦‚æœå·²ç»åœ¨ç¡®è®¤æ¨¡å¼ï¼Œç»§ç»­ç¡®è®¤æµç¨‹
            
            # ç¡®è®¤æ¨¡å¼è®¡æ•°
            if confirm_mode:
                if new_count == 0:
                    confirm_remaining -= 1
                    print(f"      ç¡®è®¤ä¸­... å‰©ä½™ {confirm_remaining} æ¬¡")
                    if confirm_remaining <= 0:
                        print(f"   âœ… ç¡®è®¤å®Œæˆï¼š{confirm_trigger_reason}")
                        break
                # å¦‚æœæœ‰æ–°æ•°æ®ï¼Œä¸Šé¢å·²ç»é€€å‡ºç¡®è®¤æ¨¡å¼äº†

            # æ›´æ–°çŠ¶æ€ï¼Œå‡†å¤‡ä¸‹ä¸€è½®
            prev_scroll_top = current_scroll_top
            prev_scroll_height = current_scroll_height

        return list(all_items.values())

    def _check_if_really_done(self, ws_url: str, no_new_count: int,
                               scroll_percent: float, prev_scroll_top: float,
                               all_duplicates_in_round: bool,
                               current_round: int = 0,
                               min_rounds: int = 10,
                               height_grew: bool = False,
                               current_height: int = 0,
                               stuck_count: int = 0,
                               all_visible_crawled: bool = False,
                               hit_bottom: bool = False,
                               actual_scrolled: int = 0,
                               visible_count: int = 0) -> dict:
        """
        å¤šé‡æ¡ä»¶è”åˆåˆ¤å®šæ˜¯å¦çœŸæ­£åˆ°è¾¾åº•éƒ¨

        æ ¸å¿ƒé€»è¾‘ï¼šå¿…é¡»æ»¡è¶³ã€å¿…è¦æ¡ä»¶ã€‘+ ã€å¤šä¸ªå……åˆ†æ¡ä»¶ã€‘æ‰ç»“æŸ

        å¿…è¦æ¡ä»¶ï¼ˆå¿…é¡»æ»¡è¶³ï¼‰ï¼š
        - è¾¾åˆ°æœ€å°æ»šåŠ¨æ¬¡æ•° (current_round >= min_rounds)

        ç»“æŸä¿¡å·ï¼ˆå¤šæ¡ä»¶ç»„åˆåˆ¤å®šï¼‰ï¼š
        - å¼ºä¿¡å·ï¼šhitBottom + all_visible_crawled + æ»šåŠ¨ç™¾åˆ†æ¯”é«˜
        - ä¸­ä¿¡å·ï¼šè¿ç»­å¤šè½®æ— æ–°æ•°æ® + all_visible_crawled + é¡µé¢é«˜åº¦ç¨³å®š
        - å¼±ä¿¡å·ï¼šè¿ç»­å¤šè½®æ— æ–°æ•°æ® + æ»šåŠ¨ç™¾åˆ†æ¯”å¾ˆé«˜ + æ— åŠ è½½æŒ‡ç¤ºå™¨

        Args:
            visible_count: å½“å‰å¯è§æ¨æ–‡æ•°é‡ï¼ˆç”¨äºåˆ¤æ–­è™šæ‹Ÿæ»šåŠ¨æ˜¯å¦å¸è½½äº†å¤ªå¤šå†…å®¹ï¼‰

        Returns:
            {'done': bool, 'reason': str, 'confidence': 'high'|'medium'|'low'}
        """

        # === å¿…è¦æ¡ä»¶æ£€æŸ¥ ===
        if current_round < min_rounds:
            return {'done': False, 'reason': f'æœªè¾¾åˆ°æœ€å°æ»šåŠ¨æ¬¡æ•° ({current_round}/{min_rounds})', 'confidence': 'none'}

        # å¦‚æœå¯è§æ¨æ–‡æ•°é‡å¾ˆå°‘ï¼ˆè™šæ‹Ÿæ»šåŠ¨å¸è½½äº†å¤§éƒ¨åˆ†å†…å®¹ï¼‰ï¼Œè¦æ›´è°¨æ…
        too_few_visible = visible_count <= 2 and current_round < min_rounds + 5

        # === æ”¶é›†å„ç§ä¿¡å· ===
        signals = {
            'no_new_for_3_rounds': no_new_count >= 3,
            'no_new_for_2_rounds': no_new_count >= 2,
            'all_duplicates': all_duplicates_in_round,
            'height_stable': not height_grew,
            'all_visible_crawled': all_visible_crawled,
            'hit_bottom': hit_bottom,
            'high_scroll_percent': scroll_percent >= 85,
            'very_high_scroll_percent': scroll_percent >= 95,
            'stuck': stuck_count >= 2,
            'too_few_visible': too_few_visible,
            'small_scroll': actual_scrolled < 100
        }

        # === å¼ºä¿¡å·åˆ¤å®šï¼šå‡ ä¹å¯ä»¥ç¡®å®šåˆ°åº• ===
        # å¿…é¡»åŒæ—¶æ»¡è¶³ï¼šåˆ°åº• + æ‰€æœ‰å¯è§å·²æŠ“å– + (æ»šåŠ¨ç™¾åˆ†æ¯”é«˜ æˆ– æ»šä¸åŠ¨)
        if signals['hit_bottom'] and signals['all_visible_crawled']:
            if signals['high_scroll_percent'] or signals['small_scroll']:
                return {
                    'done': True,
                    'reason': f'å¼ºä¿¡å·ï¼šæ»šåŠ¨åˆ°åº•éƒ¨(æ»šåŠ¨{actual_scrolled}px, {scroll_percent:.1f}%)ä¸”æ‰€æœ‰å¯è§æ¨æ–‡å·²æŠ“å–({visible_count}æ¡)',
                    'confidence': 'high'
                }

        # === ä¸­ä¿¡å·åˆ¤å®šï¼šæ¯”è¾ƒç¡®å®šåˆ°åº• ===
        # å¿…é¡»åŒæ—¶æ»¡è¶³ï¼šè¿ç»­3è½®æ— æ–° + æ‰€æœ‰å¯è§å·²æŠ“å– + é¡µé¢é«˜åº¦ç¨³å®š + ä¸æ˜¯å¤ªå°‘å¯è§
        if signals['no_new_for_3_rounds'] and signals['all_visible_crawled'] and signals['height_stable']:
            if not signals['too_few_visible']:
                return {
                    'done': True,
                    'reason': f'ä¸­ä¿¡å·ï¼šè¿ç»­3è½®æ— æ–°æ•°æ®ï¼Œæ‰€æœ‰å¯è§æ¨æ–‡å·²æŠ“å–({visible_count}æ¡)ï¼Œé¡µé¢ç¨³å®š',
                    'confidence': 'medium'
                }

        # === å¼±ä¿¡å·åˆ¤å®šï¼šå¯èƒ½åˆ°åº• ===
        # éœ€è¦å¤šä¸ªæ¡ä»¶ç»„åˆï¼Œä¸”ä¸èƒ½æœ‰åå‘ä¿¡å·
        weak_score = 0
        weak_conditions = [
            signals['no_new_for_2_rounds'],
            signals['all_duplicates'],
            signals['height_stable'],
            signals['high_scroll_percent'],
            signals['stuck'],
            signals['all_visible_crawled']
        ]
        weak_score = sum(weak_conditions)

        # å¼±ä¿¡å·éœ€è¦è‡³å°‘5ä¸ªæ¡ä»¶ï¼ˆä»¥å‰æ˜¯4ä¸ªï¼Œç°åœ¨æé«˜é˜ˆå€¼å› ä¸ºä¼šè¿›å…¥ç¡®è®¤æ¨¡å¼ï¼‰
        if weak_score >= 5 and not signals['too_few_visible']:
            # é¢å¤–æ£€æŸ¥ï¼šæ˜¯å¦æœ‰åŠ è½½æŒ‡ç¤ºå™¨
            is_loading = self._eval_js(ws_url, """
                (function() {
                    const loaders = document.querySelectorAll([
                        '[role="progressbar"]',
                        '.loading',
                        '[data-testid="loading"]',
                        'svg[class*="loading"]',
                        'div[class*="skeleton"]'
                    ].join(','));
                    return Array.from(loaders).some(l => {
                        const rect = l.getBoundingClientRect();
                        return rect.top >= 0 && rect.top <= window.innerHeight;
                    });
                })()
            """) or False

            if not is_loading:
                return {
                    'done': True,
                    'reason': f'å¼±ä¿¡å·ï¼šæ»¡è¶³{weak_score}/6ä¸ªç»“æŸæ¡ä»¶ï¼Œæ— åŠ è½½æŒ‡ç¤ºå™¨',
                    'confidence': 'low'
                }

        # === æ–‡æœ¬ç»“æŸæ ‡è®°æ£€æµ‹ ===
        end_marker = self._eval_js(ws_url, """
            (function() {
                const markers = [
                    'æ²¡æœ‰æ›´å¤šæ¨æ–‡', 'No more tweets', 'End of timeline',
                    'å·²æ˜¾ç¤ºæ‰€æœ‰æ¨æ–‡', 'All tweets shown', 'That\'s all for now',
                    'Nothing more to see', 'You\'re all caught up'
                ];
                const allText = document.body.innerText || '';
                return markers.some(m => allText.includes(m));
            })()
        """)

        if end_marker and signals['no_new_for_2_rounds']:
            return {
                'done': True,
                'reason': 'æ£€æµ‹åˆ°"æ²¡æœ‰æ›´å¤šæ¨æ–‡"æ–‡æœ¬æç¤º',
                'confidence': 'high'
            }

        # === è¿”å›æœªå®Œæˆçš„è¯¦ç»†åŸå›  ===
        true_signals = [k for k, v in signals.items() if v and k != 'too_few_visible']

        return {
            'done': False,
            'reason': f'æ¡ä»¶ä¸æ»¡è¶³ï¼ˆçœŸä¿¡å·:{len(true_signals)}/9, å¼±è¯„åˆ†:{weak_score}/6ï¼‰',
            'confidence': 'none'
        }

    def _make_progress_bar(self, percent: float, width: int = 20) -> str:
        """åˆ›å»ºè¿›åº¦æ¡"""
        filled = int(width * percent / 100)
        bar = 'â–ˆ' * filled + 'â–‘' * (width - filled)
        return f"[{bar}] {percent:5.1f}%"
        # åœ¨å‰ min_rounds è½®ï¼Œå³ä½¿åªçœ‹åˆ°é‡å¤å†…å®¹ä¹Ÿä¸åœæ­¢
        # è¿™è§£å†³äº†"é•¿æ¨æ–‡å±•å¼€åå æ®æ•´ä¸ªè§†å£"çš„é—®é¢˜
        if current_round < min_rounds:
            return {'done': False, 'reason': f'æœªè¾¾åˆ°æœ€å°æ»šåŠ¨æ¬¡æ•° ({current_round}/{min_rounds})'}

        # === è§†å£é¡¶éƒ¨æ¨æ–‡å¡ä½æ£€æµ‹ ===
        # å¦‚æœè¿ç»­å¤šè½®è§†å£é¡¶éƒ¨çš„æ¨æ–‡éƒ½æ˜¯åŒä¸€ä¸ªï¼Œè¯´æ˜æˆ‘ä»¬å¯èƒ½å¡åœ¨ä¸€ä¸ªå¾ˆé•¿çš„æ¨æ–‡é‡Œ
        # ä½†å¦‚æœé¡µé¢é«˜åº¦è¿˜åœ¨å¢é•¿ï¼Œè¯´æ˜æ¨æ–‡ä¸‹æ–¹æœ‰æ–°å†…å®¹ï¼Œä¸è¦åœæ­¢
        if stuck_count >= 3 and not height_grew:
            # å°è¯•å¼ºåˆ¶æ»šåŠ¨åˆ°ä¸‹ä¸€ä¸ªæ¨æ–‡
            forced_scroll = self._eval_js(ws_url, """
                (function() {
                    const articles = document.querySelectorAll('article[data-testid="tweet"]');
                    const viewportTop = window.pageYOffset || document.documentElement.scrollTop;

                    for (let i = 0; i < articles.length; i++) {
                        const rect = articles[i].getBoundingClientRect();
                        const articleTop = rect.top + viewportTop;

                        // æ‰¾åˆ°ç¬¬ä¸€ä¸ªå®Œå…¨åœ¨è§†å£ä¸‹æ–¹çš„æ¨æ–‡ï¼Œæ»šåŠ¨åˆ°å®ƒ
                        if (articleTop > viewportTop + window.innerHeight * 0.3) {
                            window.scrollTo({
                                top: articleTop,
                                behavior: 'smooth'
                            });
                            return {scrolled: true, target: i};
                        }
                    }
                    return {scrolled: false};
                })()
            """) or {}

            if forced_scroll.get('scrolled'):
                return {'done': False, 'reason': f'å°è¯•å¼ºåˆ¶æ»šåŠ¨åˆ°ä¸‹ä¸€ä¸ªæ¨æ–‡'}

        # === é¡µé¢é«˜åº¦è¿˜åœ¨å¢é•¿ ===
        # å¦‚æœé¡µé¢æ€»é«˜åº¦è¿˜åœ¨å¢åŠ ï¼Œè¯´æ˜æœ‰æ–°å†…å®¹åœ¨åŠ è½½ï¼Œä¸è¦åœæ­¢
        if height_grew:
            return {'done': False, 'reason': 'é¡µé¢é«˜åº¦ä»åœ¨å¢é•¿ï¼Œç»§ç»­æ»šåŠ¨'}

        # === æ£€æµ‹æ˜¯å¦åœ¨åŠ è½½ä¸­ ===
        is_loading = self._eval_js(ws_url, """
            (function() {
                // æ£€æŸ¥å„ç§åŠ è½½æŒ‡ç¤ºå™¨
                const loaders = document.querySelectorAll([
                    '[role="progressbar"]',
                    '.loading',
                    '[data-testid="loading"]',
                    'svg[class*="loading"]',
                    'div[class*="skeleton"]',
                    '[data-testid="trend"]'
                ].join(','));
                const hasVisibleLoader = Array.from(loaders).some(l => {
                    const rect = l.getBoundingClientRect();
                    return rect.top >= 0 && rect.top <= window.innerHeight;
                });

                // æ£€æŸ¥æ˜¯å¦æœ‰"åŠ è½½æ›´å¤š"æŒ‰é’®
                const loadMoreBtns = document.querySelectorAll('span, button');
                let hasLoadMore = false;
                for (const btn of loadMoreBtns) {
                    const text = (btn.innerText || '').toLowerCase();
                    if (text.includes('load more') || text.includes('åŠ è½½æ›´å¤š') ||
                        text.includes('show more replies') || text.includes('æ˜¾ç¤ºæ›´å¤šå›å¤')) {
                        hasLoadMore = true;
                        break;
                    }
                }

                return {isLoading: hasVisibleLoader, hasLoadMore: hasLoadMore};
            })()
        """) or {}

        if is_loading.get('isLoading'):
            return {'done': False, 'reason': 'æ£€æµ‹åˆ°åŠ è½½æŒ‡ç¤ºå™¨'}

        if is_loading.get('hasLoadMore'):
            return {'done': False, 'reason': 'æ£€æµ‹åˆ°"åŠ è½½æ›´å¤š"æŒ‰é’®'}

        # === æ¡ä»¶1: è¿ç»­å¤šæ¬¡æ‰€æœ‰æ¨æ–‡éƒ½æ˜¯é‡å¤çš„ + é¡µé¢é«˜åº¦ç¨³å®š ===
        # æ³¨æ„ï¼šéœ€è¦åœ¨è¾¾åˆ°æœ€å°æ»šåŠ¨æ¬¡æ•°åæ‰åˆ¤æ–­
        if no_new_count >= 3 and all_duplicates_in_round and not height_grew:
            # å†æ¬¡æ£€æŸ¥æ»šåŠ¨ä½ç½®æ˜¯å¦å˜åŒ–ï¼ˆç­‰å¾…ä¸€å°æ®µæ—¶é—´ï¼‰
            time.sleep(0.5)
            new_info = self._get_scroll_info(ws_url)
            new_scroll_top = new_info.get('scrollTop', 0)

            # å¦‚æœæ»šåŠ¨ä½ç½®è¿˜åœ¨å˜åŒ–ï¼Œè¯´æ˜æ­£åœ¨æ»šåŠ¨é•¿æ¨æ–‡å†…éƒ¨
            if abs(new_scroll_top - prev_scroll_top) > 50:
                return {'done': False, 'reason': 'æ»šåŠ¨ä½ç½®ä»åœ¨å˜åŒ–ï¼Œå¯èƒ½æ­£åœ¨é•¿æ¨æ–‡å†…éƒ¨æ»šåŠ¨'}

            return {'done': True, 'reason': f'è¿ç»­{no_new_count}è½®æ— æ–°æ•°æ®ä¸”é¡µé¢é«˜åº¦ç¨³å®š'}

        # === æ¡ä»¶2: æ»šåŠ¨ç™¾åˆ†æ¯”å¾ˆé«˜ + è¿ç»­å¤šæ¬¡æ‰€æœ‰æ¨æ–‡éƒ½æ˜¯é‡å¤çš„ ===
        if scroll_percent >= 95 and no_new_count >= 2 and all_duplicates_in_round and not height_grew:
            return {'done': True, 'reason': f'å·²æ»šåŠ¨åˆ°{scroll_percent:.1f}%ä¸”è¿ç»­{no_new_count}è½®æ— æ–°æ•°æ®'}

        # === æ¡ä»¶3: æ»šåŠ¨åˆ°åº•éƒ¨ + æ‰€æœ‰å¯è§æ¨æ–‡éƒ½å·²æŠ“å– + æ»šä¸åŠ¨ ===
        # è¿™æ˜¯æœ€å¼ºçš„å®Œæˆä¿¡å·ï¼šé¡µé¢æ»šä¸åŠ¨äº†ï¼Œä¸”æ‰€æœ‰å¯è§å†…å®¹éƒ½å·²æŠ“å–
        if hit_bottom and all_visible_crawled and actual_scrolled < 100:
            return {'done': True, 'reason': f'æ»šåŠ¨åˆ°åº•éƒ¨ä¸”æ‰€æœ‰å¯è§æ¨æ–‡å·²æŠ“å–ï¼ˆå®é™…æ»šåŠ¨{actual_scrolled}pxï¼‰'}

        # === æ¡ä»¶4: æ»šåŠ¨åˆ°åº•éƒ¨ + è¿ç»­å¤šè½®æ— æ–°æ•°æ® ===
        if hit_bottom and no_new_count >= 2 and all_duplicates_in_round:
            return {'done': True, 'reason': f'æ»šåŠ¨åˆ°åº•éƒ¨ä¸”è¿ç»­{no_new_count}è½®æ— æ–°æ•°æ®'}

        # === æ¡ä»¶5: æ‰€æœ‰å¯è§æ¨æ–‡éƒ½å·²æŠ“å– + é¡µé¢é«˜åº¦ç¨³å®š + è¿ç»­å¤šè½®æ— æ–°æ•°æ® ===
        if all_visible_crawled and not height_grew and no_new_count >= 2:
            return {'done': True, 'reason': f'æ‰€æœ‰å¯è§æ¨æ–‡å·²æŠ“å–ï¼ˆ{visible_check["visible_count"] if "visible_check" in dir() else "N"}æ¡ï¼‰ä¸”é¡µé¢ç¨³å®š'}

        # === æ¡ä»¶6: æ£€æŸ¥æ˜¯å¦å‡ºç°"æ²¡æœ‰æ›´å¤šæ¨æ–‡"çš„æç¤º ===
        end_marker = self._eval_js(ws_url, """
            (function() {
                const markers = [
                    'æ²¡æœ‰æ›´å¤šæ¨æ–‡', 'No more tweets', 'End of timeline',
                    'å·²æ˜¾ç¤ºæ‰€æœ‰æ¨æ–‡', 'All tweets shown', 'That\'s all for now'
                ];
                const allText = document.body.innerText || '';
                return markers.some(m => allText.includes(m));
            })()
        """)

        if end_marker:
            return {'done': True, 'reason': 'æ£€æµ‹åˆ°"æ²¡æœ‰æ›´å¤šæ¨æ–‡"æç¤º'}

        return {'done': False, 'reason': ''}

    def _make_progress_bar(self, percent: float, width: int = 20) -> str:
        """åˆ›å»ºè¿›åº¦æ¡"""
        filled = int(width * percent / 100)
        bar = 'â–ˆ' * filled + 'â–‘' * (width - filled)
        return f"[{bar}] {percent:5.1f}%"
        """åˆ›å»ºè¿›åº¦æ¡"""
        filled = int(width * percent / 100)
        bar = 'â–ˆ' * filled + 'â–‘' * (width - filled)
        return f"[{bar}] {percent:5.1f}%"

    def _download_via_chrome(self, ws_url: str, url: str, save_path: Path) -> bool:
        """
        é€šè¿‡ Chrome ä¸‹è½½æ–‡ä»¶ï¼ˆå¤ç”¨å½“å‰é¡µé¢çš„ Cookie å’Œè®¤è¯ï¼‰

        Args:
            ws_url: WebSocket è°ƒè¯• URL
            url: è¦ä¸‹è½½çš„æ–‡ä»¶ URL
            save_path: ä¿å­˜è·¯å¾„

        Returns:
            æ˜¯å¦ä¸‹è½½æˆåŠŸ
        """
        try:
            import base64
            import websocket

            # ä½¿ç”¨ Chrome çš„ Fetch æˆ– Network åŸŸæ¥è·å–èµ„æº
            # æ–¹æ³•ï¼šé€šè¿‡ Network.loadNetworkResource æˆ–æ‰§è¡Œ JS è·å– blob
            js_code = f"""
            (async function() {{
                try {{
                    const response = await fetch('{url}', {{
                        credentials: 'include',
                        headers: {{
                            'Accept': 'image/webp,image/apng,image/*,*/*;q=0.8'
                        }}
                    }});
                    if (response.ok) {{
                        const blob = await response.blob();
                        const reader = new FileReader();
                        return new Promise((resolve) => {{
                            reader.onloadend = () => resolve(reader.result);
                            reader.readAsDataURL(blob);
                        }});
                    }}
                    return null;
                }} catch(e) {{
                    return null;
                }}
            }})()
            """

            result = self._eval_js(ws_url, js_code, timeout=60)

            if result and result.startswith('data:'):
                # è§£ç  base64 æ•°æ®
                base64_data = result.split(',')[1]
                binary_data = base64.b64decode(base64_data)

                save_path.parent.mkdir(parents=True, exist_ok=True)
                with open(save_path, 'wb') as f:
                    f.write(binary_data)
                return True

        except Exception as e:
            print(f"      âš ï¸ ä¸‹è½½å¤±è´¥ {url}: {e}")

        return False

    def download_media(self, data: List[Dict], media_field: str = 'image_urls',
                       output_subdir: str = 'media') -> Dict[str, int]:
        """
        ä¸‹è½½æ¨æ–‡ä¸­åŒ…å«çš„åª’ä½“æ–‡ä»¶

        Args:
            data: æŠ“å–çš„æ•°æ®åˆ—è¡¨
            media_field: åŒ…å«åª’ä½“URLçš„å­—æ®µå
            output_subdir: åª’ä½“æ–‡ä»¶ä¿å­˜å­ç›®å½•

        Returns:
            ä¸‹è½½ç»Ÿè®¡ä¿¡æ¯ {'success': x, 'failed': y}
        """
        print(f"\nğŸ“¥ å¼€å§‹ä¸‹è½½åª’ä½“æ–‡ä»¶...")

        media_dir = self.output_dir / output_subdir
        media_dir.mkdir(exist_ok=True)

        stats = {'success': 0, 'failed': 0}

        for item in data:
            urls_str = item.get(media_field, '')
            if not urls_str:
                continue

            # å¤„ç†å¯èƒ½çš„å¤šURLï¼ˆé€—å·åˆ†éš”ï¼‰
            urls = [u.strip() for u in str(urls_str).split(',') if u.strip()]

            for url in urls:
                # ä» URL æå–æ–‡ä»¶å
                from urllib.parse import urlparse
                parsed = urlparse(url)
                filename = parsed.path.split('/')[-1] or 'unknown'

                # å¦‚æœæ²¡æœ‰æ‰©å±•åï¼Œæ·»åŠ  .jpg
                if '.' not in filename:
                    filename += '.jpg'

                # æ·»åŠ æ¨æ–‡IDå‰ç¼€ï¼Œé¿å…é‡å
                tweet_id = str(item.get('id', 'unknown'))[:20]
                filename = f"{tweet_id}_{filename}"

                save_path = media_dir / filename

                # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡
                if save_path.exists():
                    print(f"      â­ï¸ å·²å­˜åœ¨: {filename}")
                    stats['success'] += 1
                    continue

                print(f"      ä¸‹è½½: {filename}")

                # å°è¯•é€šè¿‡ Chrome ä¸‹è½½
                # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦ ws_urlï¼Œä½†æ•°æ®å·²ç»æå–å®Œäº†
                # æ‰€ä»¥éœ€è¦ä¿®æ”¹é€»è¾‘ï¼Œæˆ–è€…åœ¨æå–æ—¶åŒæ—¶ä¸‹è½½
                # ç®€åŒ–æ–¹æ¡ˆï¼šç›´æ¥ requests ä¸‹è½½ï¼Œæ·»åŠ  headers
                try:
                    import requests
                    headers = {
                        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
                        'Accept': 'image/webp,image/apng,image/*,*/*;q=0.8',
                        'Referer': 'https://x.com/'
                    }
                    resp = requests.get(url, headers=headers, timeout=30)
                    if resp.status_code == 200:
                        with open(save_path, 'wb') as f:
                            f.write(resp.content)
                        stats['success'] += 1
                    else:
                        stats['failed'] += 1
                        print(f"      âŒ HTTP {resp.status_code}")
                except Exception as e:
                    stats['failed'] += 1
                    print(f"      âŒ é”™è¯¯: {e}")

        print(f"\nğŸ“Š ä¸‹è½½å®Œæˆ: {stats['success']} æˆåŠŸ, {stats['failed']} å¤±è´¥")
        print(f"   ä¿å­˜ä½ç½®: {media_dir}")

        return stats

    def save(self, data: List[Dict], name: str, config: ExtractorConfig = None):
        """
        ä¿å­˜æ•°æ®åˆ°å¤šç§æ ¼å¼
        æµç¨‹: 1. ä¿å­˜åŸå§‹JSON 2. ä»JSONç”ŸæˆCSV 3. ä»JSONç”ŸæˆMarkdown
        """
        if not data:
            print("âŒ æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return

        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        base_name = f"{name}_{timestamp}"

        # æ’åº
        if config and config.sort_field:
            from datetime import datetime as dt
            
            def get_sort_key(item):
                value = item.get(config.sort_field, '')
                if not value:
                    return ''
                
                # å°è¯•è§£æ ISO 8601 æ—¶é—´æ ¼å¼ (2024-02-06T15:30:00.000Z)
                if isinstance(value, str):
                    # å°è¯•å¤šç§æ—¶é—´æ ¼å¼
                    time_formats = [
                        '%Y-%m-%dT%H:%M:%S.%fZ',
                        '%Y-%m-%dT%H:%M:%SZ',
                        '%Y-%m-%dT%H:%M:%S.%f%z',
                        '%Y-%m-%dT%H:%M:%S%z',
                        '%Y-%m-%d %H:%M:%S',
                        '%Y-%m-%d'
                    ]
                    for fmt in time_formats:
                        try:
                            parsed = dt.strptime(value, fmt)
                            # è¿”å›æ—¶é—´æˆ³ç”¨äºæ’åº
                            return parsed.timestamp()
                        except ValueError:
                            continue
                
                # å¦‚æœæ— æ³•è§£æä¸ºæ—¶é—´ï¼ŒæŒ‰åŸå€¼å­—ç¬¦ä¸²æ’åº
                return str(value)
            
            data.sort(key=get_sort_key, reverse=config.sort_reverse)

        # ===== 1. ä¿å­˜åŸå§‹ JSONï¼ˆæœ€æƒå¨çš„æ•°æ®æºï¼‰ =====
        json_file = self.output_dir / f"{base_name}.json"
        json_content = {
            'source': name,
            'crawled_at': datetime.now().isoformat(),
            'count': len(data),
            'data': data
        }
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(json_content, f, ensure_ascii=False, indent=2)

        # ===== 2. ä» JSON ç”Ÿæˆ CSVï¼ˆç®€åŒ–æ ¼å¼ï¼‰ =====
        csv_file = self.output_dir / f"{base_name}.csv"
        self._generate_csv_from_json(json_content, csv_file, config)

        # ===== 3. ä» JSON ç”Ÿæˆ Markdownï¼ˆå¯è¯»æ ¼å¼ï¼‰ =====
        md_file = self.output_dir / f"{base_name}.md"
        self._generate_md_from_json(json_content, md_file, name)

        print(f"\nâœ… å·²ä¿å­˜:")
        print(f"   ğŸ“„ JSON (åŸå§‹æ•°æ®): {json_file}")
        print(f"   ğŸ“Š CSV (è¡¨æ ¼è§†å›¾): {csv_file}")
        print(f"   ğŸ“ Markdown (å¯è¯»æ ¼å¼): {md_file}")

        return json_file, csv_file, md_file

    def _generate_csv_from_json(self, json_content: Dict, csv_file: Path, config: ExtractorConfig = None):
        """ä» JSON å†…å®¹ç”Ÿæˆ CSV æ–‡ä»¶"""
        data = json_content.get('data', [])
        if not data:
            return

        import csv

        # å®šä¹‰ CSV è¦åŒ…å«çš„å­—æ®µï¼ˆä¼˜å…ˆä½¿ç”¨é…ç½®ï¼Œå¦åˆ™ä½¿ç”¨æ•°æ®ä¸­æ‰€æœ‰å­—æ®µï¼‰
        if config and config.field_selectors:
            # åªåŒ…å«é…ç½®ä¸­å®šä¹‰çš„å­—æ®µ + åª’ä½“ç›¸å…³å­—æ®µ
            base_fields = list(config.field_selectors.keys())
            media_fields = ['image_count', 'has_video', 'image_urls']
            fieldnames = [f for f in (base_fields + media_fields) if f in data[0] or f in media_fields]
        else:
            # ä½¿ç”¨æ•°æ®ä¸­æ‰€æœ‰å­—æ®µï¼Œä½†æ’é™¤å†…éƒ¨å­—æ®µ
            fieldnames = [k for k in data[0].keys() if not k.startswith('_')]

        with open(csv_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction='ignore')
            writer.writeheader()

            for item in data:
                # åˆ›å»º CSV è¡Œï¼Œå¤„ç†é•¿æ–‡æœ¬æˆªæ–­
                row = {}
                for field in fieldnames:
                    value = item.get(field, '')
                    # æ–‡æœ¬å­—æ®µæˆªæ–­ï¼Œé¿å… CSV è¿‡é•¿
                    if field in ['text', 'content'] and isinstance(value, str) and len(value) > 500:
                        value = value[:497] + '...'
                    row[field] = value
                writer.writerow(row)

    def _generate_md_from_json(self, json_content: Dict, md_file: Path, name: str):
        """ä» JSON å†…å®¹ç”Ÿæˆ Markdown æ–‡ä»¶"""
        data = json_content.get('data', [])
        meta = {
            'source': json_content.get('source', name),
            'crawled_at': json_content.get('crawled_at', ''),
            'count': json_content.get('count', 0)
        }

        with open(md_file, 'w', encoding='utf-8') as f:
            # æ ‡é¢˜å’Œå…ƒä¿¡æ¯
            f.write(f"# {meta['source']} æ•°æ®\n\n")
            f.write(f"- **æŠ“å–æ—¶é—´**: {meta['crawled_at'][:19].replace('T', ' ')}\n")
            f.write(f"- **æ•°æ®æ¡æ•°**: {meta['count']}\n")
            f.write(f"- **åŸå§‹æ•°æ®**: è§åŒå `.json` æ–‡ä»¶\n\n")
            f.write("---\n\n")

            # åªå±•ç¤ºå‰ 100 æ¡
            display_count = min(len(data), 100)
            for i, item in enumerate(data[:display_count], 1):
                # æ ‡é¢˜ï¼šä¼˜å…ˆä½¿ç”¨ text å­—æ®µå‰ 50 å­—
                title_text = item.get('text', item.get('title', 'æ— æ ‡é¢˜'))[:50]
                if len(item.get('text', '')) > 50:
                    title_text += '...'

                f.write(f"### {i}. {title_text}\n\n")

                # å†…å®¹å­—æ®µ
                if 'text' in item:
                    f.write(f"**å†…å®¹**:\n```\n{item['text']}\n```\n\n")

                # å…¶ä»–å­—æ®µè¡¨æ ¼
                other_fields = {k: v for k, v in item.items()
                               if not k.startswith('_') and k != 'text' and v}
                if other_fields:
                    f.write("| å­—æ®µ | å†…å®¹ |\n|------|------|\n")
                    for key, value in list(other_fields.items())[:10]:  # æœ€å¤šæ˜¾ç¤º10ä¸ªå­—æ®µ
                        value_str = str(value)[:100]  # æˆªæ–­é•¿å†…å®¹
                        if len(str(value)) > 100:
                            value_str += '...'
                        f.write(f"| {key} | {value_str} |\n")
                    f.write("\n")

                f.write("---\n\n")

            if len(data) > 100:
                f.write(f"\n> å…± {len(data)} æ¡æ•°æ®ï¼Œæ­¤å¤„ä»…å±•ç¤ºå‰ 100 æ¡ã€‚å®Œæ•´æ•°æ®è¯·æŸ¥çœ‹ JSON æ–‡ä»¶ã€‚\n")


# ============ é¢„è®¾é…ç½® ============

class Presets:
    """å¸¸ç”¨ç½‘ç«™é¢„è®¾é…ç½®"""

    @staticmethod
    def twitter(username: str, download_media: bool = False) -> ExtractorConfig:
        """Twitter/X æ¨æ–‡æŠ“å–"""

        def extract_full_text(element_html: str) -> str:
            """æå–å®Œæ•´æ¨æ–‡æ–‡æœ¬ï¼Œå¤„ç†å±•å¼€åçš„é•¿æ–‡æœ¬"""
            return element_html.strip()

        return ExtractorConfig(
            name=f"Twitter @{username}",
            url_pattern=rf"x\.com/{username}",
            item_selector='article[data-testid="tweet"]',
            field_selectors={
                'id': 'a[href*="/status/"]',
                'text': '[data-testid="tweetText"]',
                'time': 'time',
                'author': 'div[data-testid="User-Name"] a',
                'likes': '[data-testid="like"]',
                'replies': '[data-testid="reply"]',
                'retweets': '[data-testid="retweet"]'
            },
            scroll_delay=2.5,
            expand_selectors=[
                '[data-testid="tweet-text-show-more-link"]',
            ],
            expand_delay=1.5,
            download_media=download_media,
            field_processors={
                'id': lambda x: re.search(r'/status/(\d+)', str(x)).group(1) if re.search(r'/status/(\d+)', str(x)) else x,
                'likes': lambda x: int(re.search(r'(\d+)', str(x).replace(',', '')).group(1)) if re.search(r'(\d+)', str(x)) else 0,
                'replies': lambda x: int(re.search(r'(\d+)', str(x).replace(',', '')).group(1)) if re.search(r'(\d+)', str(x)) else 0,
                'retweets': lambda x: int(re.search(r'(\d+)', str(x).replace(',', '')).group(1)) if re.search(r'(\d+)', str(x)) else 0,
            },
            sort_field='time'
        )

    @staticmethod
    def zhihu_answers() -> ExtractorConfig:
        """çŸ¥ä¹å›ç­”æŠ“å–"""
        return ExtractorConfig(
            name="çŸ¥ä¹å›ç­”",
            url_pattern=r"zhihu\.com/question/\d+",
            item_selector='.AnswerCard, .ContentItem.AnswerItem',
            field_selectors={
                'author': '.AuthorInfo-name',
                'content': '.RichContent-inner',
                'votes': '.VoteButton--up',
                'comments': '.ContentItem-action:has(.CommentIcon)'
            },
            expand_selectors=['.ContentItem-more', '.RichContent-inner--collapsed']
        )

    @staticmethod
    def douban_reviews() -> ExtractorConfig:
        """è±†ç“£å½±è¯„/ä¹¦è¯„æŠ“å–"""
        return ExtractorConfig(
            name="è±†ç“£è¯„è®º",
            url_pattern=r"douban\.com/subject/\d+/reviews",
            item_selector='.review-item',
            field_selectors={
                'title': '.main-bd h2 a',
                'author': '.main-hd .name',
                'rating': '.main-title-rating',
                'content': '.short-content',
                'votes': '.action-btn.up span'
            }
        )

    @staticmethod
    def github_issues() -> ExtractorConfig:
        """GitHub Issues æŠ“å–"""
        return ExtractorConfig(
            name="GitHub Issues",
            url_pattern=r"github\.com/[^/]+/[^/]+/issues",
            item_selector='[data-testid="issue-row"]',
            field_selectors={
                'title': 'a[data-testid="issue-title"]',
                'number': 'span[title]',
                'status': '[data-testid="issue-row-status"]',
                'author': '[data-testid="issue-row-author"]'
            },
            scroll_enabled=False  # GitHub ç”¨åˆ†é¡µï¼Œä¸ç”¨æ»šåŠ¨
        )


# ============ ä½¿ç”¨ç¤ºä¾‹ ============

def main():
    """æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨æ¡†æ¶"""
    import sys

    # æ£€æŸ¥å‚æ•°
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•:")
        print(f"  python3 {sys.argv[0]} <preset> [options]")
        print("")
        print("å¯ç”¨é¢„è®¾:")
        print("  twitter <username>  - æŠ“å– Twitter æ¨æ–‡")
        print("  zhihu               - æŠ“å–çŸ¥ä¹å›ç­”")
        print("  douban              - æŠ“å–è±†ç“£è¯„è®º")
        print("  github              - æŠ“å– GitHub Issues")
        print("")
        print("ç¤ºä¾‹:")
        print(f"  python3 {sys.argv[0]} twitter elonmusk")
        return

    preset = sys.argv[1]
    spider = CDPSpider()

    # æ ¹æ®é¢„è®¾åˆ›å»ºé…ç½®
    if preset == 'twitter':
        username = sys.argv[2] if len(sys.argv) > 2 else input("è¾“å…¥ Twitter ç”¨æˆ·å: ")
        config = Presets.twitter(username)
        # ä½¿ç”¨ç”¨æˆ·åä½œä¸ºæ–‡ä»¶åå‰ç¼€
        preset = username
    elif preset == 'zhihu':
        config = Presets.zhihu_answers()
    elif preset == 'douban':
        config = Presets.douban_reviews()
    elif preset == 'github':
        config = Presets.github_issues()
    else:
        print(f"âŒ æœªçŸ¥é¢„è®¾: {preset}")
        return

    # æ‰§è¡ŒæŠ“å–
    data = spider.crawl(config)

    if data:
        spider.save(data, preset, config)
        print(f"\nğŸ‰ å®Œæˆ! å…±æŠ“å– {len(data)} æ¡æ•°æ®")
    else:
        print("\nâŒ æŠ“å–å¤±è´¥")


if __name__ == '__main__':
    main()
