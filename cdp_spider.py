#!/usr/bin/env python3
"""
CDP Spider - é€šç”¨ç½‘é¡µæŠ“å–æ¡†æ¶
åŸºäº Chrome DevTools Protocol çš„çµæ´»æ•°æ®æå–å·¥å…·

ç‰¹ç‚¹ï¼š
- é€šè¿‡é…ç½®æ–‡ä»¶å®šä¹‰æŠ“å–é€»è¾‘
- æ”¯æŒæ»šåŠ¨åŠ è½½ã€åˆ†é¡µã€ç‚¹å‡»å±•å¼€
- å¤šç§æ•°æ®å¯¼å‡ºæ ¼å¼
- å†…ç½®å¸¸è§ç½‘ç«™é¢„è®¾é…ç½®

ä½œè€…: 0xC1A
"""

import json
import requests
import time
import re
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional, Callable
from dataclasses import dataclass, field


@dataclass
class ExtractorConfig:
    """æ•°æ®æå–é…ç½®"""
    name: str                          # æå–å™¨åç§°
    url_pattern: str                   # URL åŒ¹é…æ¨¡å¼
    
    # é€‰æ‹©å™¨é…ç½®
    item_selector: str                 # åˆ—è¡¨é¡¹é€‰æ‹©å™¨ (å¦‚: 'article[data-testid="tweet"]')
    field_selectors: Dict[str, str]    # å­—æ®µé€‰æ‹©å™¨ {å­—æ®µå: CSSé€‰æ‹©å™¨}
    
    # æ»šåŠ¨/åˆ†é¡µé…ç½®
    scroll_enabled: bool = True        # æ˜¯å¦å¯ç”¨æ»šåŠ¨
    scroll_times: int = 50             # æœ€å¤§æ»šåŠ¨æ¬¡æ•°
    scroll_delay: float = 2.0          # æ»šåŠ¨é—´éš”(ç§’)
    scroll_selector: Optional[str] = None  # æ»šåŠ¨å®¹å™¨é€‰æ‹©å™¨ (Noneåˆ™æ»šåŠ¨æ•´ä¸ªé¡µé¢)
    
    # å±•å¼€é…ç½®
    expand_selectors: List[str] = field(default_factory=list)  # éœ€è¦ç‚¹å‡»å±•å¼€çš„å…ƒç´ 
    expand_delay: float = 1.0          # å±•å¼€åç­‰å¾…æ—¶é—´
    
    # æ•°æ®å¤„ç†
    field_processors: Dict[str, Callable] = field(default_factory=dict)  # å­—æ®µåå¤„ç†å™¨
    item_filter: Optional[Callable] = None  # é¡¹ç›®è¿‡æ»¤å‡½æ•°
    
    # å¯¼å‡ºé…ç½®
    id_field: str = 'id'               # å”¯ä¸€æ ‡è¯†å­—æ®µ
    sort_field: str = ''               # æ’åºå­—æ®µ
    sort_reverse: bool = True          # å€’åºæ’åº


class CDPSpider:
    """CDP æŠ“å–æ¡†æ¶ä¸»ç±»"""
    
    def __init__(self, chrome_port: int = 9222, output_dir: str = 'spider_exports'):
        self.chrome_port = chrome_port
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
    def _check_chrome(self) -> tuple[bool, str]:
        """æ£€æŸ¥ Chrome DevTools è¿æ¥"""
        try:
            resp = requests.get(f'http://localhost:{self.chrome_port}/json/version', timeout=5)
            if resp.status_code == 200:
                return True, resp.json().get('Browser', 'unknown')
        except:
            pass
        return False, ''
    
    def _get_page(self, url_pattern: str) -> Optional[Dict]:
        """è·å–åŒ¹é…çš„é¡µé¢"""
        try:
            resp = requests.get(f'http://localhost:{self.chrome_port}/json/list', timeout=10)
            pages = resp.json()
            
            for p in pages:
                page_url = p.get('url', '')
                if re.search(url_pattern, page_url) and 'devtools' not in page_url:
                    return {
                        'id': p['id'],
                        'url': page_url,
                        'ws_url': p['webSocketDebuggerUrl'],
                        'title': p.get('title', 'Unknown')
                    }
        except Exception as e:
            print(f"âŒ è·å–é¡µé¢åˆ—è¡¨å¤±è´¥: {e}")
        return None
    
    def _eval_js(self, ws_url: str, js_code: str, timeout: int = 30) -> Any:
        """æ‰§è¡Œ JavaScript"""
        try:
            import websocket
            ws = websocket.create_connection(ws_url, timeout=timeout)
            ws.send(json.dumps({
                'id': 1,
                'method': 'Runtime.evaluate',
                'params': {
                    'expression': js_code,
                    'returnByValue': True,
                    'awaitPromise': True
                }
            }))
            result = ws.recv()
            ws.close()
            
            data = json.loads(result)
            if 'result' in data and 'result' in data['result']:
                return data['result']['result'].get('value')
        except Exception as e:
            print(f"  âš ï¸ JS æ‰§è¡Œé”™è¯¯: {e}")
        return None
    
    def _expand_items(self, ws_url: str, config: ExtractorConfig):
        """ç‚¹å‡»å±•å¼€æ‰€æœ‰æŠ˜å é¡¹ - ä»…å±•å¼€é•¿æ–‡æœ¬ï¼Œä¸è·³è½¬é¡µé¢"""
        for selector in config.expand_selectors:
            # å¤šæ¬¡å°è¯•ï¼Œç›´åˆ°æ²¡æœ‰æ–°çš„å¯å±•å¼€é¡¹
            for attempt in range(3):
                js_code = f"""
                (function() {{
                    // åªåœ¨å½“å‰é¡µé¢ï¼ˆæ—¶é—´çº¿ï¼‰æ‰§è¡Œï¼Œä¸åœ¨æ¨æ–‡è¯¦æƒ…é¡µæ‰§è¡Œ
                    if (window.location.pathname.includes('/status/')) {{
                        return -1; // æ ‡è®°ä¸ºåœ¨é”™è¯¯é¡µé¢
                    }}
                    
                    const items = document.querySelectorAll('{selector}');
                    let clicked = 0;
                    items.forEach(item => {{
                        // ä¸¥æ ¼æ£€æŸ¥ï¼šå¯è§ã€æœªè¢«ç‚¹å‡»è¿‡ã€ä¸”æ–‡æœ¬ç²¾ç¡®åŒ¹é…
                        if (item && item.offsetParent !== null && !item.getAttribute('data-expanded')) {{
                            const text = (item.innerText || item.textContent || '').trim().toLowerCase();
                            const ariaLabel = (item.getAttribute('aria-label') || '').toLowerCase();
                            
                            // åªç‚¹å‡»çœŸæ­£çš„ "Show more" æŒ‰é’®
                            const isShowMore = text === 'show more' || 
                                              ariaLabel === 'show more' ||
                                              item.getAttribute('data-testid') === 'tweet-text-show-more-link';
                            
                            if (isShowMore) {{
                                item.setAttribute('data-expanded', 'true');
                                item.click();
                                clicked++;
                            }}
                        }}
                    }});
                    return clicked;
                }})()
                """
                result = self._eval_js(ws_url, js_code)
                
                if result == -1:
                    print(f"      âš ï¸ æ£€æµ‹åˆ°åœ¨æ¨æ–‡è¯¦æƒ…é¡µï¼Œè·³è¿‡å±•å¼€æ“ä½œ")
                    return
                
                clicked = int(result) if isinstance(result, (int, float)) else 0
                if clicked > 0:
                    print(f"      å±•å¼€ {clicked} ä¸ªæŠ˜å é¡¹ (å°è¯• {attempt + 1})")
                    time.sleep(config.expand_delay)
                else:
                    break
    
    def _scroll_page(self, ws_url: str, config: ExtractorConfig):
        """æ»šåŠ¨é¡µé¢"""
        if config.scroll_selector:
            # æ»šåŠ¨ç‰¹å®šå®¹å™¨
            js_code = f"""
                document.querySelector('{config.scroll_selector}').scrollTop = 
                document.querySelector('{config.scroll_selector}').scrollHeight;
            """
        else:
            # æ»šåŠ¨æ•´ä¸ªé¡µé¢
            js_code = "window.scrollTo({top: document.body.scrollHeight, behavior: 'smooth'});"
        
        self._eval_js(ws_url, js_code)
        time.sleep(config.scroll_delay)
    
    def _extract_items(self, ws_url: str, config: ExtractorConfig) -> List[Dict]:
        """æå–å½“å‰é¡µé¢çš„æ‰€æœ‰é¡¹ç›®"""
        # å…ˆå±•å¼€æŠ˜å é¡¹
        if config.expand_selectors:
            self._expand_items(ws_url, config)
        
        # æ„å»ºæå– JS
        field_extractors = []
        for field_name, selector in config.field_selectors.items():
            field_extractors.append(f"""
                // {field_name}
                try {{
                    const {field_name}El = article.querySelector('{selector}');
                    if ({field_name}El) {{
                        // ä¼˜å…ˆä½¿ç”¨ innerText è·å–æ¸²æŸ“åçš„æ–‡æœ¬ï¼ˆåŒ…å«å±•å¼€åçš„å†…å®¹ï¼‰
                        let text = {field_name}El.innerText || {field_name}El.textContent || '';
                        // ä¹Ÿå°è¯•ä» href è·å–é“¾æ¥
                        if (!text && {field_name}El.getAttribute('href')) {{
                            text = {field_name}El.getAttribute('href');
                        }}
                        // ä¹Ÿå°è¯• aria-label
                        if (!text && {field_name}El.getAttribute('aria-label')) {{
                            text = {field_name}El.getAttribute('aria-label');
                        }}
                        item['{field_name}'] = text.trim();
                    }}
                }} catch(e) {{}}
            """)
        
        js_code = f"""
        (function() {{
            const items = [];
            const articles = document.querySelectorAll('{config.item_selector}');
            
            articles.forEach((article, index) => {{
                try {{
                    const item = {{_index: index}};
                    {''.join(field_extractors)}
                    items.push(item);
                }} catch(e) {{}}
            }});
            
            return items;
        }})()
        """
        
        result = self._eval_js(ws_url, js_code)
        return result if isinstance(result, list) else []
    
    def crawl(self, config: ExtractorConfig) -> List[Dict]:
        """
        æ‰§è¡ŒæŠ“å–
        
        Args:
            config: æå–å™¨é…ç½®
            
        Returns:
            æŠ“å–çš„æ•°æ®åˆ—è¡¨
        """
        print("=" * 70)
        print(f"ğŸ•·ï¸  CDP Spider - {config.name}")
        print("=" * 70)
        
        # æ£€æŸ¥ Chrome
        print("\nğŸ“¡ è¿æ¥ Chrome...")
        connected, browser = self._check_chrome()
        if not connected:
            print("âŒ æ— æ³•è¿æ¥åˆ° Chrome")
            print(f"\nè¯·å…ˆå¯åŠ¨ Chrome:")
            print(f"  /Applications/Google\\ Chrome.app/Contents/MacOS/Google\\ Chrome \\")
            print(f"      --remote-debugging-port={self.chrome_port} \\")
            print(f"      --remote-allow-origins='*' \\")
            print(f"      --user-data-dir=/tmp/chrome_dev_profile")
            return []
        print(f"âœ… å·²è¿æ¥ ({browser})")
        
        # æŸ¥æ‰¾ç›®æ ‡é¡µé¢
        print(f"\nğŸ“„ æŸ¥æ‰¾é¡µé¢: {config.url_pattern}")
        page = self._get_page(config.url_pattern)
        if not page:
            print("âŒ æœªæ‰¾åˆ°åŒ¹é…çš„é¡µé¢")
            print("è¯·åœ¨ Chrome ä¸­æ‰“å¼€ç›®æ ‡é¡µé¢")
            return []
        print(f"âœ… æ‰¾åˆ°é¡µé¢: {page['title'][:50]}")
        
        # å¼€å§‹æŠ“å–
        print(f"\nğŸ” å¼€å§‹æŠ“å–...")
        if config.scroll_enabled:
            print(f"   æ»šåŠ¨æ¨¡å¼: æœ€å¤š {config.scroll_times} æ¬¡")
        
        all_items = {}
        ws_url = page['ws_url']
        
        for i in range(config.scroll_times if config.scroll_enabled else 1):
            # æå–æ•°æ®
            items = self._extract_items(ws_url, config)
            
            new_count = 0
            for item in items:
                item_id = item.get(config.id_field) or item.get('_index')
                if item_id and item_id not in all_items:
                    # åº”ç”¨å­—æ®µå¤„ç†å™¨
                    for field, processor in config.field_processors.items():
                        if field in item:
                            item[field] = processor(item[field])
                    
                    # åº”ç”¨è¿‡æ»¤å™¨
                    if config.item_filter is None or config.item_filter(item):
                        all_items[item_id] = item
                        new_count += 1
            
            if i % 5 == 0 or new_count > 0:
                print(f"   ç¬¬ {i+1} è½®: +{new_count} æ¡æ–°æ•°æ®, æ€»è®¡: {len(all_items)} æ¡")
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç»§ç»­
            if not config.scroll_enabled:
                break
                
            if new_count == 0 and i > 5:
                print(f"   âœ… æ²¡æœ‰æ–°æ•°æ®äº†ï¼Œåœæ­¢")
                break
            
            # æ»šåŠ¨
            self._scroll_page(ws_url, config)
        
        return list(all_items.values())
    
    def save(self, data: List[Dict], name: str, config: ExtractorConfig = None):
        """ä¿å­˜æ•°æ®åˆ°å¤šç§æ ¼å¼"""
        if not data:
            print("âŒ æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        base_name = f"{name}_{timestamp}"
        
        # æ’åº
        if config and config.sort_field:
            data.sort(key=lambda x: x.get(config.sort_field, ''), 
                     reverse=config.sort_reverse)
        
        # JSON
        json_file = self.output_dir / f"{base_name}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump({
                'source': name,
                'crawled_at': datetime.now().isoformat(),
                'count': len(data),
                'data': data
            }, f, ensure_ascii=False, indent=2)
        
        # CSV
        if data and isinstance(data[0], dict):
            import csv
            csv_file = self.output_dir / f"{base_name}.csv"
            with open(csv_file, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=data[0].keys())
                writer.writeheader()
                writer.writerows(data)
        
        # Markdown
        md_file = self.output_dir / f"{base_name}.md"
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write(f"# {name} æ•°æ®\n\n")
            f.write(f"æŠ“å–æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æ•°æ®æ¡æ•°: {len(data)}\n\n")
            f.write("---\n\n")
            
            for i, item in enumerate(data[:100], 1):  # åªæ˜¾ç¤ºå‰100æ¡
                f.write(f"### {i}. {item.get('title', item.get('text', 'Item'))[:50]}\n\n")
                for key, value in item.items():
                    if not key.startswith('_'):
                        f.write(f"- **{key}**: {value}\n")
                f.write("\n---\n\n")
        
        print(f"\nâœ… å·²ä¿å­˜:")
        print(f"   ğŸ“„ JSON: {json_file}")
        print(f"   ğŸ“Š CSV: {csv_file}")
        print(f"   ğŸ“ Markdown: {md_file}")


# ============ é¢„è®¾é…ç½® ============

class Presets:
    """å¸¸ç”¨ç½‘ç«™é¢„è®¾é…ç½®"""
    
    @staticmethod
    def twitter(username: str) -> ExtractorConfig:
        """Twitter/X æ¨æ–‡æŠ“å–"""
        
        def extract_full_text(element_html: str) -> str:
            """æå–å®Œæ•´æ¨æ–‡æ–‡æœ¬ï¼Œå¤„ç†å±•å¼€åçš„é•¿æ–‡æœ¬"""
            # è¿™ä¸ªå¤„ç†å™¨ä¼šåœ¨ JS æ‰§è¡Œåé€šè¿‡ innerText è·å–
            # ä½†å¦‚æœè¿˜æœ‰é—®é¢˜ï¼Œå¯ä»¥åœ¨è¿™é‡Œåšåå¤„ç†
            return element_html.strip()
        
        return ExtractorConfig(
            name=f"Twitter @{username}",
            url_pattern=rf"x\.com/{username}",
            item_selector='article[data-testid="tweet"]',
            field_selectors={
                'id': 'a[href*="/status/"]',
                'text': '[data-testid="tweetText"]',  # å±•å¼€åä¼šè‡ªåŠ¨åŒ…å«å®Œæ•´æ–‡æœ¬
                'time': 'time',
                'author': 'div[data-testid="User-Name"] a',
                'likes': '[data-testid="like"]',
                'replies': '[data-testid="reply"]',
                'retweets': '[data-testid="retweet"]'
            },
            scroll_times=50,
            scroll_delay=2.5,  # ç¨å¾®å¢åŠ æ»šåŠ¨é—´éš”
            expand_selectors=[
                '[data-testid="tweet-text-show-more-link"]',  # Twitter å®˜æ–¹çš„é•¿æ–‡æœ¬å±•å¼€æŒ‰é’®
            ],
            expand_delay=1.5,  # å¢åŠ å±•å¼€åç­‰å¾…æ—¶é—´
            field_processors={
                'id': lambda x: re.search(r'/status/(\d+)', str(x)).group(1) if re.search(r'/status/(\d+)', str(x)) else x,
                'likes': lambda x: int(re.search(r'(\d+)', str(x).replace(',', '')).group(1)) if re.search(r'(\d+)', str(x)) else 0,
                'replies': lambda x: int(re.search(r'(\d+)', str(x).replace(',', '')).group(1)) if re.search(r'(\d+)', str(x)) else 0,
                'retweets': lambda x: int(re.search(r'(\d+)', str(x).replace(',', '')).group(1)) if re.search(r'(\d+)', str(x)) else 0,
            },
            sort_field='time'
        )
    
    @staticmethod
    def zhihu_answers() -> ExtractorConfig:
        """çŸ¥ä¹å›ç­”æŠ“å–"""
        return ExtractorConfig(
            name="çŸ¥ä¹å›ç­”",
            url_pattern=r"zhihu\.com/question/\d+",
            item_selector='.AnswerCard, .ContentItem.AnswerItem',
            field_selectors={
                'author': '.AuthorInfo-name',
                'content': '.RichContent-inner',
                'votes': '.VoteButton--up',
                'comments': '.ContentItem-action:has(.CommentIcon)'
            },
            scroll_times=30,
            expand_selectors=['.ContentItem-more', '.RichContent-inner--collapsed']
        )
    
    @staticmethod
    def douban_reviews() -> ExtractorConfig:
        """è±†ç“£å½±è¯„/ä¹¦è¯„æŠ“å–"""
        return ExtractorConfig(
            name="è±†ç“£è¯„è®º",
            url_pattern=r"douban\.com/subject/\d+/reviews",
            item_selector='.review-item',
            field_selectors={
                'title': '.main-bd h2 a',
                'author': '.main-hd .name',
                'rating': '.main-title-rating',
                'content': '.short-content',
                'votes': '.action-btn.up span'
            },
            scroll_times=20
        )
    
    @staticmethod
    def github_issues() -> ExtractorConfig:
        """GitHub Issues æŠ“å–"""
        return ExtractorConfig(
            name="GitHub Issues",
            url_pattern=r"github\.com/[^/]+/[^/]+/issues",
            item_selector='[data-testid="issue-row"]',
            field_selectors={
                'title': 'a[data-testid="issue-title"]',
                'number': 'span[title]',
                'status': '[data-testid="issue-row-status"]',
                'author': '[data-testid="issue-row-author"]'
            },
            scroll_enabled=False  # GitHub ç”¨åˆ†é¡µï¼Œä¸ç”¨æ»šåŠ¨
        )


# ============ ä½¿ç”¨ç¤ºä¾‹ ============

def main():
    """æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨æ¡†æ¶"""
    import sys
    
    # æ£€æŸ¥å‚æ•°
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•:")
        print(f"  python3 {sys.argv[0]} <preset> [options]")
        print("")
        print("å¯ç”¨é¢„è®¾:")
        print("  twitter <username>  - æŠ“å– Twitter æ¨æ–‡")
        print("  zhihu               - æŠ“å–çŸ¥ä¹å›ç­”")
        print("  douban              - æŠ“å–è±†ç“£è¯„è®º")
        print("  github              - æŠ“å– GitHub Issues")
        print("")
        print("ç¤ºä¾‹:")
        print(f"  python3 {sys.argv[0]} twitter elonmusk")
        return
    
    preset = sys.argv[1]
    spider = CDPSpider()
    
    # æ ¹æ®é¢„è®¾åˆ›å»ºé…ç½®
    if preset == 'twitter':
        username = sys.argv[2] if len(sys.argv) > 2 else input("è¾“å…¥ Twitter ç”¨æˆ·å: ")
        config = Presets.twitter(username)
    elif preset == 'zhihu':
        config = Presets.zhihu_answers()
    elif preset == 'douban':
        config = Presets.douban_reviews()
    elif preset == 'github':
        config = Presets.github_issues()
    else:
        print(f"âŒ æœªçŸ¥é¢„è®¾: {preset}")
        return
    
    # æ‰§è¡ŒæŠ“å–
    data = spider.crawl(config)
    
    if data:
        spider.save(data, preset, config)
        print(f"\nğŸ‰ å®Œæˆ! å…±æŠ“å– {len(data)} æ¡æ•°æ®")
    else:
        print("\nâŒ æŠ“å–å¤±è´¥")


if __name__ == '__main__':
    main()
