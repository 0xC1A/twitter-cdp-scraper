#!/usr/bin/env python3
"""
CDP Spider - é€šç”¨ç½‘é¡µæŠ“å–æ¡†æ¶
åŸºäº Chrome DevTools Protocol çš„çµæ´»æ•°æ®æå–å·¥å…·

ç‰¹ç‚¹ï¼š
- é€šè¿‡é…ç½®æ–‡ä»¶å®šä¹‰æŠ“å–é€»è¾‘
- æ”¯æŒæ»šåŠ¨åŠ è½½ã€åˆ†é¡µã€ç‚¹å‡»å±•å¼€
- æ™ºèƒ½æ»šåŠ¨ç­–ç•¥åº”å¯¹è™šæ‹Ÿæ»šåŠ¨ï¼ˆå¦‚ Twitter/Xï¼‰
- å¤šç§æ•°æ®å¯¼å‡ºæ ¼å¼
- å†…ç½®å¸¸è§ç½‘ç«™é¢„è®¾é…ç½®

ä½œè€…: 0xC1A
"""

import json
import requests
import time
import re
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional, Callable
from dataclasses import dataclass, field


@dataclass
class ExtractorConfig:
    """æ•°æ®æå–é…ç½®"""
    name: str                          # æå–å™¨åç§°
    url_pattern: str                   # URL åŒ¹é…æ¨¡å¼
    
    # é€‰æ‹©å™¨é…ç½®
    item_selector: str                 # åˆ—è¡¨é¡¹é€‰æ‹©å™¨ (å¦‚: 'article[data-testid="tweet"]')
    field_selectors: Dict[str, str]    # å­—æ®µé€‰æ‹©å™¨ {å­—æ®µå: CSSé€‰æ‹©å™¨}
    
    # æ»šåŠ¨/åˆ†é¡µé…ç½®
    scroll_enabled: bool = True        # æ˜¯å¦å¯ç”¨æ»šåŠ¨
    scroll_times: int = 50             # æœ€å¤§æ»šåŠ¨æ¬¡æ•°
    scroll_delay: float = 2.0          # æ»šåŠ¨é—´éš”(ç§’)
    scroll_selector: Optional[str] = None  # æ»šåŠ¨å®¹å™¨é€‰æ‹©å™¨ (Noneåˆ™æ»šåŠ¨æ•´ä¸ªé¡µé¢)
    
    # å±•å¼€é…ç½®
    expand_selectors: List[str] = field(default_factory=list)  # éœ€è¦ç‚¹å‡»å±•å¼€çš„å…ƒç´ 
    expand_delay: float = 1.0          # å±•å¼€åç­‰å¾…æ—¶é—´
    
    # åª’ä½“ä¸‹è½½é…ç½®
    download_media: bool = False       # æ˜¯å¦ä¸‹è½½åª’ä½“æ–‡ä»¶
    
    # æ•°æ®å¤„ç†
    field_processors: Dict[str, Callable] = field(default_factory=dict)  # å­—æ®µåå¤„ç†å™¨
    item_filter: Optional[Callable] = None  # é¡¹ç›®è¿‡æ»¤å‡½æ•°
    
    # å¯¼å‡ºé…ç½®
    id_field: str = 'id'               # å”¯ä¸€æ ‡è¯†å­—æ®µ
    sort_field: str = ''               # æ’åºå­—æ®µ
    sort_reverse: bool = True          # å€’åºæ’åº


class CDPSpider:
    """CDP æŠ“å–æ¡†æ¶ä¸»ç±»"""
    
    def __init__(self, chrome_port: int = 9222, output_dir: str = 'spider_exports'):
        self.chrome_port = chrome_port
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
    def _check_chrome(self) -> tuple[bool, str]:
        """æ£€æŸ¥ Chrome DevTools è¿æ¥"""
        try:
            resp = requests.get(f'http://localhost:{self.chrome_port}/json/version', timeout=5)
            if resp.status_code == 200:
                return True, resp.json().get('Browser', 'unknown')
        except:
            pass
        return False, ''
    
    def _get_page(self, url_pattern: str) -> Optional[Dict]:
        """è·å–åŒ¹é…çš„é¡µé¢"""
        try:
            resp = requests.get(f'http://localhost:{self.chrome_port}/json/list', timeout=10)
            pages = resp.json()
            
            for p in pages:
                page_url = p.get('url', '')
                if re.search(url_pattern, page_url) and 'devtools' not in page_url:
                    return {
                        'id': p['id'],
                        'url': page_url,
                        'ws_url': p['webSocketDebuggerUrl'],
                        'title': p.get('title', 'Unknown')
                    }
        except Exception as e:
            print(f"âŒ è·å–é¡µé¢åˆ—è¡¨å¤±è´¥: {e}")
        return None
    
    def _eval_js(self, ws_url: str, js_code: str, timeout: int = 30) -> Any:
        """æ‰§è¡Œ JavaScript"""
        try:
            import websocket
            ws = websocket.create_connection(ws_url, timeout=timeout)
            ws.send(json.dumps({
                'id': 1,
                'method': 'Runtime.evaluate',
                'params': {
                    'expression': js_code,
                    'returnByValue': True,
                    'awaitPromise': True
                }
            }))
            result = ws.recv()
            ws.close()
            
            data = json.loads(result)
            if 'result' in data and 'result' in data['result']:
                return data['result']['result'].get('value')
        except Exception as e:
            print(f"  âš ï¸ JS æ‰§è¡Œé”™è¯¯: {e}")
        return None
    
    def _expand_items(self, ws_url: str, config: ExtractorConfig):
        """ç‚¹å‡»å±•å¼€æ‰€æœ‰æŠ˜å é¡¹ - ä»…å±•å¼€ä¸»æ¨æ–‡çš„é•¿æ–‡æœ¬ï¼Œé¿å…ç‚¹å‡»å¼•ç”¨æ¨æ–‡å¯¼è‡´è·³è½¬"""
        for selector in config.expand_selectors:
            for attempt in range(5):  # å¢åŠ å°è¯•æ¬¡æ•°
                js_code = f"""
                (function() {{
                    // æ£€æŸ¥æ˜¯å¦åœ¨æ—¶é—´çº¿é¡µé¢
                    if (window.location.pathname.includes('/status/')) {{
                        return {{status: 'wrong_page', msg: 'åœ¨æ¨æ–‡è¯¦æƒ…é¡µ'}};
                    }}
                    
                    let clicked = 0;
                    
                    // æ–¹æ³•1: é€šè¿‡ data-testid æŸ¥æ‰¾
                    const items1 = document.querySelectorAll('[data-testid="tweet-text-show-more-link"]');
                    
                    // æ–¹æ³•2: é€šè¿‡æ–‡æœ¬å†…å®¹æŸ¥æ‰¾æ‰€æœ‰åŒ…å« "Show more" çš„ span/button
                    // ä¸»æ¨æ–‡çš„ show more é€šå¸¸æ˜¯ tweetText åŒºåŸŸå†…çš„ span
                    const allArticles = document.querySelectorAll('article[data-testid="tweet"]');
                    
                    // ä¼˜å…ˆä½¿ç”¨æ–¹æ³•1
                    items1.forEach(item => {{
                        if (!item || item.offsetParent === null || item.getAttribute('data-expanded')) {{
                            return;
                        }}
                        
                        // æ£€æŸ¥æ˜¯å¦åœ¨ä¸»æ¨æ–‡å†…ï¼ˆä¸æ˜¯å¼•ç”¨æ¨æ–‡ï¼‰
                        // å¼•ç”¨æ¨æ–‡é€šå¸¸åœ¨ä¸€ä¸ªåµŒå¥—çš„ article æˆ–ç‰¹å®šå®¹å™¨å†…
                        const isQuoteTweet = item.closest('div[role="link"]') !== null || 
                                            item.closest('[data-testid="quotedTweet"]') !== null ||
                                            item.closest('article') !== item.closest('article[data-testid="tweet"]');
                        
                        if (isQuoteTweet) {{
                            return;
                        }}
                        
                        item.setAttribute('data-expanded', 'true');
                        item.click();
                        clicked++;
                    }});
                    
                    // å¦‚æœæ–¹æ³•1æ²¡ç‚¹åˆ°ï¼Œå°è¯•æ–¹æ³•2ï¼šåœ¨æ¯ä¸ª article å†…æŸ¥æ‰¾ show more
                    if (clicked === 0) {{
                        allArticles.forEach(article => {{
                            // åªå¤„ç†ä¸»æ¨æ–‡çš„ tweetText åŒºåŸŸ
                            const tweetText = article.querySelector('[data-testid="tweetText"]');
                            if (!tweetText) return;
                            
                            // åœ¨ tweetText å†…æŸ¥æ‰¾ show more æŒ‰é’®
                            // å®ƒå¯èƒ½æ˜¯ä¸€ä¸ª span æˆ– buttonï¼ŒåŒ…å« "Show more" æ–‡æœ¬
                            const allElements = tweetText.querySelectorAll('span, button');
                            
                            allElements.forEach(el => {{
                                if (el.getAttribute('data-expanded')) return;
                                
                                const text = (el.innerText || el.textContent || '').trim();
                                const ariaLabel = (el.getAttribute('aria-label') || '').trim();
                                
                                // åŒ¹é… Show moreï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
                                if (text.toLowerCase() === 'show more' || 
                                    ariaLabel.toLowerCase() === 'show more' ||
                                    text.toLowerCase().includes('show more')) {{
                                    
                                    el.setAttribute('data-expanded', 'true');
                                    el.click();
                                    clicked++;
                                }}
                            }});
                        }});
                    }}
                    
                    return {{status: 'success', clicked: clicked}};
                }})()
                """
                result = self._eval_js(ws_url, js_code)
                
                if isinstance(result, dict):
                    if result.get('status') == 'wrong_page':
                        print(f"      âš ï¸ æ£€æµ‹åˆ°åœ¨æ¨æ–‡è¯¦æƒ…é¡µï¼Œåœæ­¢å±•å¼€")
                        return
                    clicked = result.get('clicked', 0)
                else:
                    clicked = int(result) if isinstance(result, (int, float)) else 0
                
                if clicked > 0:
                    print(f"      å±•å¼€ {clicked} ä¸ªä¸»æ¨æ–‡æŠ˜å é¡¹ (å°è¯• {attempt + 1})")
                    time.sleep(config.expand_delay)
                else:
                    break
    
    def _scroll_page(self, ws_url: str, config: ExtractorConfig, step: int = 1):
        """
        æ»šåŠ¨é¡µé¢ - ä½¿ç”¨å°æ­¥é•¿æ»šåŠ¨é¿å…è™šæ‹Ÿæ»šåŠ¨å¯¼è‡´çš„æ•°æ®ä¸¢å¤±
        
        Args:
            step: æ»šåŠ¨æ­¥æ•°ï¼Œæ¯æ¬¡æ»šåŠ¨ä¸€å±çš„ä¸€éƒ¨åˆ†
        """
        # è®¡ç®—æ»šåŠ¨è·ç¦»ï¼šè§†å£é«˜åº¦çš„ 70%ï¼Œç¡®ä¿æœ‰é‡å åŒºåŸŸ
        js_code = """
        (function() {
            const viewportHeight = window.innerHeight;
            const scrollDistance = Math.floor(viewportHeight * 0.7);
            const currentScroll = window.pageYOffset || document.documentElement.scrollTop;
            
            window.scrollTo({
                top: currentScroll + scrollDistance,
                behavior: 'smooth'
            });
            
            return {
                scrolled: scrollDistance,
                viewportHeight: viewportHeight,
                newPosition: currentScroll + scrollDistance,
                pageHeight: document.body.scrollHeight
            };
        })()
        """
        
        result = self._eval_js(ws_url, js_code)
        time.sleep(config.scroll_delay)
        return result
    
    def _get_scroll_info(self, ws_url: str) -> dict:
        """è·å–å½“å‰æ»šåŠ¨ä¿¡æ¯"""
        js_code = """
        (function() {
            return {
                scrollTop: window.pageYOffset || document.documentElement.scrollTop,
                scrollHeight: document.body.scrollHeight,
                viewportHeight: window.innerHeight,
                scrollPercent: ((window.pageYOffset || document.documentElement.scrollTop) / 
                               (document.body.scrollHeight - window.innerHeight) * 100).toFixed(1)
            };
        })()
        """
        return self._eval_js(ws_url, js_code) or {}
    
    def _extract_items(self, ws_url: str, config: ExtractorConfig, download_media: bool = False, media_dir: Path = None) -> List[Dict]:
        """æå–å½“å‰é¡µé¢çš„æ‰€æœ‰é¡¹ç›®"""
        # å…ˆå±•å¼€æŠ˜å é¡¹
        if config.expand_selectors:
            self._expand_items(ws_url, config)
        
        # æ„å»ºæå– JS
        field_extractors = []
        for field_name, selector in config.field_selectors.items():
            # è·³è¿‡åª’ä½“å­—æ®µï¼Œæˆ‘ä»¬å•ç‹¬å¤„ç†
            if field_name in ['image_urls', 'video_urls']:
                continue
                
            # åˆ¤æ–­æ˜¯å¦éœ€è¦ä¼˜å…ˆè·å– hrefï¼ˆå¦‚ id å­—æ®µæˆ–é€‰æ‹©å™¨åŒ…å«é“¾æ¥ç›¸å…³ï¼‰
            prefer_href = field_name in ['id', 'url', 'link'] or 'href' in selector
            
            field_extractors.append(f"""
                // {field_name}
                try {{
                    const {field_name}El = article.querySelector('{selector}');
                    if ({field_name}El) {{
                        let text = '';
                        
                        // å¯¹äº id/url/link å­—æ®µï¼Œä¼˜å…ˆè·å– href
                        if ({str(prefer_href).lower()}) {{
                            text = {field_name}El.getAttribute('href') || '';
                            if (!text) {{
                                text = {field_name}El.innerText || {field_name}El.textContent || '';
                            }}
                        }} else {{
                            // å…¶ä»–å­—æ®µä¼˜å…ˆä½¿ç”¨ innerText
                            text = {field_name}El.innerText || {field_name}El.textContent || '';
                            if (!text) {{
                                text = {field_name}El.getAttribute('href') || '';
                            }}
                        }}
                        
                        // ä¹Ÿå°è¯• aria-label
                        if (!text) {{
                            text = {field_name}El.getAttribute('aria-label') || '';
                        }}
                        
                        item['{field_name}'] = text.trim();
                    }}
                }} catch(e) {{}}
            """)
        
        # æ·»åŠ åª’ä½“æå–ä»£ç 
        media_extractor = """
            // æå–å›¾ç‰‡ URL
            try {
                const images = article.querySelectorAll('[data-testid="tweetPhoto"] img');
                const imageUrls = Array.from(images).map(img => img.src).filter(Boolean);
                if (imageUrls.length > 0) {
                    item['image_urls'] = imageUrls.join(',');
                    item['image_count'] = imageUrls.length;
                }
            } catch(e) {}
            
            // æå–è§†é¢‘æ ‡è®°
            try {
                const video = article.querySelector('[data-testid="videoPlayer"], [data-testid="videoComponent"]');
                if (video) {
                    item['has_video'] = true;
                }
            } catch(e) {}
        """
        
        js_code = f"""
        (function() {{
            const items = [];
            const articles = document.querySelectorAll('{config.item_selector}');
            
            articles.forEach((article, index) => {{
                try {{
                    const item = {{_index: index}};
                    {''.join(field_extractors)}
                    {media_extractor}
                    items.push(item);
                }} catch(e) {{}}
            }});
            
            return items;
        }})()
        """
        
        result = self._eval_js(ws_url, js_code)
        items = result if isinstance(result, list) else []
        
        # å¦‚æœå¯ç”¨äº†åª’ä½“ä¸‹è½½ï¼ŒåŒæ—¶ä¸‹è½½å›¾ç‰‡
        if download_media and media_dir:
            for item in items:
                image_urls = item.get('image_urls', '')
                if image_urls:
                    urls = [u.strip() for u in image_urls.split(',') if u.strip()]
                    downloaded = []
                    
                    for url in urls:
                        tweet_id = str(item.get('id', 'unknown'))[:20]
                        filename = f"{tweet_id}_{url.split('/')[-1].split('?')[0]}"
                        if '.' not in filename:
                            filename += '.jpg'
                        
                        save_path = media_dir / filename
                        
                        if self._download_via_chrome(ws_url, url, save_path):
                            downloaded.append(filename)
                    
                    if downloaded:
                        item['downloaded_images'] = ','.join(downloaded)
        
        return items
    
    def crawl(self, config: ExtractorConfig) -> List[Dict]:
        """
        æ‰§è¡ŒæŠ“å– - ä½¿ç”¨æ™ºèƒ½æ»šåŠ¨ç­–ç•¥åº”å¯¹è™šæ‹Ÿæ»šåŠ¨
        
        Args:
            config: æå–å™¨é…ç½®
            
        Returns:
            æŠ“å–çš„æ•°æ®åˆ—è¡¨
        """
        print("=" * 70)
        print(f"ğŸ•·ï¸  CDP Spider - {config.name}")
        print("=" * 70)
        
        # æ£€æŸ¥ Chrome
        print("\nğŸ“¡ è¿æ¥ Chrome...")
        connected, browser = self._check_chrome()
        if not connected:
            print("âŒ æ— æ³•è¿æ¥åˆ° Chrome")
            print(f"\nè¯·å…ˆå¯åŠ¨ Chrome:")
            print(f"  /Applications/Google\\ Chrome.app/Contents/MacOS/Google\\ Chrome \\")
            print(f"      --remote-debugging-port={self.chrome_port} \\")
            print(f"      --remote-allow-origins='*' \\")
            print(f"      --user-data-dir=/tmp/chrome_dev_profile")
            return []
        print(f"âœ… å·²è¿æ¥ ({browser})")
        
        # æŸ¥æ‰¾ç›®æ ‡é¡µé¢
        print(f"\nğŸ“„ æŸ¥æ‰¾é¡µé¢: {config.url_pattern}")
        page = self._get_page(config.url_pattern)
        if not page:
            print("âŒ æœªæ‰¾åˆ°åŒ¹é…çš„é¡µé¢")
            print("è¯·åœ¨ Chrome ä¸­æ‰“å¼€ç›®æ ‡é¡µé¢")
            return []
        print(f"âœ… æ‰¾åˆ°é¡µé¢: {page['title'][:50]}")
        
        # å¼€å§‹æŠ“å–
        print(f"\nğŸ” å¼€å§‹æŠ“å–...")
        print(f"   æ»šåŠ¨ç­–ç•¥: å°æ­¥é•¿æ»šåŠ¨ + å³æ—¶æå–ï¼ˆåº”å¯¹è™šæ‹Ÿæ»šåŠ¨ï¼‰")
        print(f"   æœ€å¤§æ»šåŠ¨æ¬¡æ•°: {config.scroll_times}")
        
        all_items = {}
        ws_url = page['ws_url']
        no_new_count = 0  # è¿ç»­æ²¡æœ‰æ–°æ•°æ®çš„æ¬¡æ•°
        prev_scroll_top = 0
        
        # åª’ä½“ä¸‹è½½é…ç½®
        download_media = getattr(config, 'download_media', False)
        media_dir = None
        if download_media:
            media_dir = self.output_dir / f"media_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            media_dir.mkdir(exist_ok=True)
            print(f"   åª’ä½“ä¸‹è½½: å¯ç”¨ -> {media_dir}")
        
        # é‡ç½®æ»šåŠ¨ä½ç½®åˆ°é¡¶éƒ¨ï¼ˆç¡®ä¿ä»ç¬¬ä¸€æ¡æ¨æ–‡å¼€å§‹æŠ“å–ï¼‰
        print("\nğŸ“ é‡ç½®æ»šåŠ¨ä½ç½®åˆ°é¡¶éƒ¨...")
        self._eval_js(ws_url, "window.scrollTo({top: 0, behavior: 'instant'});")
        time.sleep(1)  # ç­‰å¾…è™šæ‹Ÿæ»šåŠ¨é‡æ–°æ¸²æŸ“
        print("   âœ… å·²å›åˆ°é¡¶éƒ¨ï¼Œå¼€å§‹æŠ“å–\n")
        
        for i in range(config.scroll_times if config.scroll_enabled else 1):
            # æå–æ•°æ®ï¼ˆåœ¨æ»šåŠ¨å‰ä¹Ÿæå–ä¸€æ¬¡ï¼Œç¡®ä¿ç¬¬ä¸€å±çš„æ•°æ®ï¼‰
            items = self._extract_items(ws_url, config, download_media, media_dir)
            
            new_count = 0
            duplicate_count = 0
            for item in items:
                item_id = item.get(config.id_field) or item.get('_index')
                if item_id:
                    if item_id not in all_items:
                        # åº”ç”¨å­—æ®µå¤„ç†å™¨
                        for field, processor in config.field_processors.items():
                            if field in item:
                                item[field] = processor(item[field])
                        
                        # åº”ç”¨è¿‡æ»¤å™¨
                        if config.item_filter is None or config.item_filter(item):
                            all_items[item_id] = item
                            new_count += 1
                    else:
                        duplicate_count += 1
            
            # è·å–æ»šåŠ¨ä¿¡æ¯
            scroll_info = self._get_scroll_info(ws_url)
            scroll_percent = float(scroll_info.get('scrollPercent', 0))
            current_scroll_top = scroll_info.get('scrollTop', 0)
            
            # æ˜¾ç¤ºè¿›åº¦
            progress_bar = self._make_progress_bar(scroll_percent)
            all_duplicates = len(items) > 0 and new_count == 0  # å½“å‰è·å–çš„æ‰€æœ‰æ¨æ–‡éƒ½æ˜¯é‡å¤çš„
            status_marker = "âœ“" if all_duplicates else " "
            print(f"   ç¬¬ {i+1:2d} è½® | {progress_bar} | "
                  f"+{new_count:3d} æ–°æ•°æ® | "
                  f"é‡å¤:{duplicate_count:2d} | "
                  f"æ€»è®¡:{len(all_items):4d} æ¡ [{status_marker}]")
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦åœæ­¢
            if not config.scroll_enabled:
                break
            
            # åˆ¤æ–­æ¡ä»¶ï¼šå½“å‰è·å–çš„æ‰€æœ‰æ¨æ–‡éƒ½å·²ç»è¢«æŠ“å–è¿‡ï¼ˆä¸”ç¡®å®è·å–åˆ°äº†æ¨æ–‡ï¼‰
            if len(items) > 0 and new_count == 0:
                no_new_count += 1
            else:
                no_new_count = 0
            
            # ä½¿ç”¨å¤šé‡æ£€æµ‹åˆ¤æ–­æ˜¯å¦çœŸæ­£å®Œæˆ
            done_check = self._check_if_really_done(
                ws_url, no_new_count, scroll_percent, prev_scroll_top, all_duplicates
            )
            
            if done_check['done']:
                print(f"   âœ… {done_check['reason']}")
                break
            
            # å°æ­¥é•¿æ»šåŠ¨
            prev_scroll_top = current_scroll_top
            self._scroll_page(ws_url, config, step=i+1)
        
        return list(all_items.values())
    
    def _check_if_really_done(self, ws_url: str, no_new_count: int, 
                               scroll_percent: float, prev_scroll_top: float,
                               all_duplicates_in_round: bool) -> dict:
        """
        å¤šé‡æ£€æµ‹åˆ¤æ–­æ˜¯å¦çœŸæ­£åˆ°è¾¾åº•éƒ¨
        
        Args:
            no_new_count: è¿ç»­Nè½®æ‰€æœ‰æ¨æ–‡éƒ½æ˜¯é‡å¤çš„æ¬¡æ•°
            all_duplicates_in_round: å½“å‰è½®æ¬¡æ‰€æœ‰æ¨æ–‡æ˜¯å¦éƒ½æ˜¯é‡å¤çš„
            
        Returns:
            {
                'done': bool,
                'reason': str
            }
        """
        # æ¡ä»¶1: è¿ç»­å¤šæ¬¡æ‰€æœ‰æ¨æ–‡éƒ½æ˜¯é‡å¤çš„ + æ»šåŠ¨ä½ç½®ä¸å†å˜åŒ–
        if no_new_count >= 3 and all_duplicates_in_round:
            current_info = self._get_scroll_info(ws_url)
            current_scroll_top = current_info.get('scrollTop', 0)
            
            # ç­‰å¾…ä¸€å°æ®µæ—¶é—´å†æ£€æŸ¥ï¼Œçœ‹æ˜¯å¦æœ‰æ–°å†…å®¹åŠ è½½
            time.sleep(0.5)
            new_info = self._get_scroll_info(ws_url)
            new_scroll_top = new_info.get('scrollTop', 0)
            
            # å¦‚æœæ»šåŠ¨ä½ç½®åŸºæœ¬ä¸å˜ï¼Œè¯´æ˜çœŸçš„åˆ°åº•äº†
            if abs(new_scroll_top - current_scroll_top) < 10:
                # å†æ£€æŸ¥ä¸€æ¬¡DOMä¸­æ˜¯å¦æœ‰åŠ è½½æŒ‡ç¤ºå™¨
                has_loader = self._eval_js(ws_url, """
                    (function() {
                        // æ£€æŸ¥å„ç§åŠ è½½æŒ‡ç¤ºå™¨
                        const loaders = document.querySelectorAll([
                            '[role="progressbar"]',
                            '.loading',
                            '[data-testid="loading"]',
                            'svg[class*="loading"]',
                            'div[class*="skeleton"]'
                        ].join(','));
                        return loaders.length > 0 && 
                               Array.from(loaders).some(l => l.offsetParent !== null);
                    })()
                """)
                
                if not has_loader:
                    return {'done': True, 'reason': f'è¿ç»­{no_new_count}è½®æ‰€æœ‰æ¨æ–‡éƒ½æ˜¯é‡å¤çš„ä¸”é¡µé¢åœæ­¢åŠ è½½'}
        
        # æ¡ä»¶2: æ»šåŠ¨ç™¾åˆ†æ¯”å¾ˆé«˜ + è¿ç»­å¤šæ¬¡æ‰€æœ‰æ¨æ–‡éƒ½æ˜¯é‡å¤çš„
        if scroll_percent >= 95 and no_new_count >= 2 and all_duplicates_in_round:
            return {'done': True, 'reason': f'å·²æ»šåŠ¨åˆ°{scroll_percent:.1f}%ä¸”è¿ç»­{no_new_count}è½®æ‰€æœ‰æ¨æ–‡éƒ½æ˜¯é‡å¤çš„'}
        
        # æ¡ä»¶3: æ£€æŸ¥æ˜¯å¦å‡ºç°"æ²¡æœ‰æ›´å¤šæ¨æ–‡"çš„æç¤º
        end_marker = self._eval_js(ws_url, """
            (function() {
                // æ£€æŸ¥å„ç§å¯èƒ½çš„ç»“æŸæç¤º
                const markers = [
                    'æ²¡æœ‰æ›´å¤šæ¨æ–‡',
                    'No more tweets',
                    'End of timeline',
                    'å·²æ˜¾ç¤ºæ‰€æœ‰æ¨æ–‡',
                    'All tweets shown'
                ];
                const allText = document.body.innerText || '';
                return markers.some(m => allText.includes(m));
            })()
        """)
        
        if end_marker:
            return {'done': True, 'reason': 'æ£€æµ‹åˆ°"æ²¡æœ‰æ›´å¤šæ¨æ–‡"æç¤º'}
        
        return {'done': False, 'reason': ''}
    
    def _make_progress_bar(self, percent: float, width: int = 20) -> str:
        """åˆ›å»ºè¿›åº¦æ¡"""
        filled = int(width * percent / 100)
        bar = 'â–ˆ' * filled + 'â–‘' * (width - filled)
        return f"[{bar}] {percent:5.1f}%"
        """åˆ›å»ºè¿›åº¦æ¡"""
        filled = int(width * percent / 100)
        bar = 'â–ˆ' * filled + 'â–‘' * (width - filled)
        return f"[{bar}] {percent:5.1f}%"
    
    def _download_via_chrome(self, ws_url: str, url: str, save_path: Path) -> bool:
        """
        é€šè¿‡ Chrome ä¸‹è½½æ–‡ä»¶ï¼ˆå¤ç”¨å½“å‰é¡µé¢çš„ Cookie å’Œè®¤è¯ï¼‰
        
        Args:
            ws_url: WebSocket è°ƒè¯• URL
            url: è¦ä¸‹è½½çš„æ–‡ä»¶ URL
            save_path: ä¿å­˜è·¯å¾„
            
        Returns:
            æ˜¯å¦ä¸‹è½½æˆåŠŸ
        """
        try:
            import base64
            import websocket
            
            # ä½¿ç”¨ Chrome çš„ Fetch æˆ– Network åŸŸæ¥è·å–èµ„æº
            # æ–¹æ³•ï¼šé€šè¿‡ Network.loadNetworkResource æˆ–æ‰§è¡Œ JS è·å– blob
            js_code = f"""
            (async function() {{
                try {{
                    const response = await fetch('{url}', {{
                        credentials: 'include',
                        headers: {{
                            'Accept': 'image/webp,image/apng,image/*,*/*;q=0.8'
                        }}
                    }});
                    if (response.ok) {{
                        const blob = await response.blob();
                        const reader = new FileReader();
                        return new Promise((resolve) => {{
                            reader.onloadend = () => resolve(reader.result);
                            reader.readAsDataURL(blob);
                        }});
                    }}
                    return null;
                }} catch(e) {{
                    return null;
                }}
            }})()
            """
            
            result = self._eval_js(ws_url, js_code, timeout=60)
            
            if result and result.startswith('data:'):
                # è§£ç  base64 æ•°æ®
                base64_data = result.split(',')[1]
                binary_data = base64.b64decode(base64_data)
                
                save_path.parent.mkdir(parents=True, exist_ok=True)
                with open(save_path, 'wb') as f:
                    f.write(binary_data)
                return True
                
        except Exception as e:
            print(f"      âš ï¸ ä¸‹è½½å¤±è´¥ {url}: {e}")
        
        return False
    
    def download_media(self, data: List[Dict], media_field: str = 'image_urls', 
                       output_subdir: str = 'media') -> Dict[str, int]:
        """
        ä¸‹è½½æ¨æ–‡ä¸­åŒ…å«çš„åª’ä½“æ–‡ä»¶
        
        Args:
            data: æŠ“å–çš„æ•°æ®åˆ—è¡¨
            media_field: åŒ…å«åª’ä½“URLçš„å­—æ®µå
            output_subdir: åª’ä½“æ–‡ä»¶ä¿å­˜å­ç›®å½•
            
        Returns:
            ä¸‹è½½ç»Ÿè®¡ä¿¡æ¯ {'success': x, 'failed': y}
        """
        print(f"\nğŸ“¥ å¼€å§‹ä¸‹è½½åª’ä½“æ–‡ä»¶...")
        
        media_dir = self.output_dir / output_subdir
        media_dir.mkdir(exist_ok=True)
        
        stats = {'success': 0, 'failed': 0}
        
        for item in data:
            urls_str = item.get(media_field, '')
            if not urls_str:
                continue
            
            # å¤„ç†å¯èƒ½çš„å¤šURLï¼ˆé€—å·åˆ†éš”ï¼‰
            urls = [u.strip() for u in str(urls_str).split(',') if u.strip()]
            
            for url in urls:
                # ä» URL æå–æ–‡ä»¶å
                from urllib.parse import urlparse
                parsed = urlparse(url)
                filename = parsed.path.split('/')[-1] or 'unknown'
                
                # å¦‚æœæ²¡æœ‰æ‰©å±•åï¼Œæ·»åŠ  .jpg
                if '.' not in filename:
                    filename += '.jpg'
                
                # æ·»åŠ æ¨æ–‡IDå‰ç¼€ï¼Œé¿å…é‡å
                tweet_id = str(item.get('id', 'unknown'))[:20]
                filename = f"{tweet_id}_{filename}"
                
                save_path = media_dir / filename
                
                # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡
                if save_path.exists():
                    print(f"      â­ï¸ å·²å­˜åœ¨: {filename}")
                    stats['success'] += 1
                    continue
                
                print(f"      ä¸‹è½½: {filename}")
                
                # å°è¯•é€šè¿‡ Chrome ä¸‹è½½
                # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦ ws_urlï¼Œä½†æ•°æ®å·²ç»æå–å®Œäº†
                # æ‰€ä»¥éœ€è¦ä¿®æ”¹é€»è¾‘ï¼Œæˆ–è€…åœ¨æå–æ—¶åŒæ—¶ä¸‹è½½
                # ç®€åŒ–æ–¹æ¡ˆï¼šç›´æ¥ requests ä¸‹è½½ï¼Œæ·»åŠ  headers
                try:
                    import requests
                    headers = {
                        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
                        'Accept': 'image/webp,image/apng,image/*,*/*;q=0.8',
                        'Referer': 'https://x.com/'
                    }
                    resp = requests.get(url, headers=headers, timeout=30)
                    if resp.status_code == 200:
                        with open(save_path, 'wb') as f:
                            f.write(resp.content)
                        stats['success'] += 1
                    else:
                        stats['failed'] += 1
                        print(f"      âŒ HTTP {resp.status_code}")
                except Exception as e:
                    stats['failed'] += 1
                    print(f"      âŒ é”™è¯¯: {e}")
        
        print(f"\nğŸ“Š ä¸‹è½½å®Œæˆ: {stats['success']} æˆåŠŸ, {stats['failed']} å¤±è´¥")
        print(f"   ä¿å­˜ä½ç½®: {media_dir}")
        
        return stats
    
    def save(self, data: List[Dict], name: str, config: ExtractorConfig = None):
        """
        ä¿å­˜æ•°æ®åˆ°å¤šç§æ ¼å¼
        æµç¨‹: 1. ä¿å­˜åŸå§‹JSON 2. ä»JSONç”ŸæˆCSV 3. ä»JSONç”ŸæˆMarkdown
        """
        if not data:
            print("âŒ æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        base_name = f"{name}_{timestamp}"
        
        # æ’åº
        if config and config.sort_field:
            data.sort(key=lambda x: x.get(config.sort_field, ''), 
                     reverse=config.sort_reverse)
        
        # ===== 1. ä¿å­˜åŸå§‹ JSONï¼ˆæœ€æƒå¨çš„æ•°æ®æºï¼‰ =====
        json_file = self.output_dir / f"{base_name}.json"
        json_content = {
            'source': name,
            'crawled_at': datetime.now().isoformat(),
            'count': len(data),
            'data': data
        }
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(json_content, f, ensure_ascii=False, indent=2)
        
        # ===== 2. ä» JSON ç”Ÿæˆ CSVï¼ˆç®€åŒ–æ ¼å¼ï¼‰ =====
        csv_file = self.output_dir / f"{base_name}.csv"
        self._generate_csv_from_json(json_content, csv_file, config)
        
        # ===== 3. ä» JSON ç”Ÿæˆ Markdownï¼ˆå¯è¯»æ ¼å¼ï¼‰ =====
        md_file = self.output_dir / f"{base_name}.md"
        self._generate_md_from_json(json_content, md_file, name)
        
        print(f"\nâœ… å·²ä¿å­˜:")
        print(f"   ğŸ“„ JSON (åŸå§‹æ•°æ®): {json_file}")
        print(f"   ğŸ“Š CSV (è¡¨æ ¼è§†å›¾): {csv_file}")
        print(f"   ğŸ“ Markdown (å¯è¯»æ ¼å¼): {md_file}")
        
        return json_file, csv_file, md_file
    
    def _generate_csv_from_json(self, json_content: Dict, csv_file: Path, config: ExtractorConfig = None):
        """ä» JSON å†…å®¹ç”Ÿæˆ CSV æ–‡ä»¶"""
        data = json_content.get('data', [])
        if not data:
            return
        
        import csv
        
        # å®šä¹‰ CSV è¦åŒ…å«çš„å­—æ®µï¼ˆä¼˜å…ˆä½¿ç”¨é…ç½®ï¼Œå¦åˆ™ä½¿ç”¨æ•°æ®ä¸­æ‰€æœ‰å­—æ®µï¼‰
        if config and config.field_selectors:
            # åªåŒ…å«é…ç½®ä¸­å®šä¹‰çš„å­—æ®µ + åª’ä½“ç›¸å…³å­—æ®µ
            base_fields = list(config.field_selectors.keys())
            media_fields = ['image_count', 'has_video', 'image_urls']
            fieldnames = [f for f in (base_fields + media_fields) if f in data[0] or f in media_fields]
        else:
            # ä½¿ç”¨æ•°æ®ä¸­æ‰€æœ‰å­—æ®µï¼Œä½†æ’é™¤å†…éƒ¨å­—æ®µ
            fieldnames = [k for k in data[0].keys() if not k.startswith('_')]
        
        with open(csv_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction='ignore')
            writer.writeheader()
            
            for item in data:
                # åˆ›å»º CSV è¡Œï¼Œå¤„ç†é•¿æ–‡æœ¬æˆªæ–­
                row = {}
                for field in fieldnames:
                    value = item.get(field, '')
                    # æ–‡æœ¬å­—æ®µæˆªæ–­ï¼Œé¿å… CSV è¿‡é•¿
                    if field in ['text', 'content'] and isinstance(value, str) and len(value) > 500:
                        value = value[:497] + '...'
                    row[field] = value
                writer.writerow(row)
    
    def _generate_md_from_json(self, json_content: Dict, md_file: Path, name: str):
        """ä» JSON å†…å®¹ç”Ÿæˆ Markdown æ–‡ä»¶"""
        data = json_content.get('data', [])
        meta = {
            'source': json_content.get('source', name),
            'crawled_at': json_content.get('crawled_at', ''),
            'count': json_content.get('count', 0)
        }
        
        with open(md_file, 'w', encoding='utf-8') as f:
            # æ ‡é¢˜å’Œå…ƒä¿¡æ¯
            f.write(f"# {meta['source']} æ•°æ®\n\n")
            f.write(f"- **æŠ“å–æ—¶é—´**: {meta['crawled_at'][:19].replace('T', ' ')}\n")
            f.write(f"- **æ•°æ®æ¡æ•°**: {meta['count']}\n")
            f.write(f"- **åŸå§‹æ•°æ®**: è§åŒå `.json` æ–‡ä»¶\n\n")
            f.write("---\n\n")
            
            # åªå±•ç¤ºå‰ 100 æ¡
            display_count = min(len(data), 100)
            for i, item in enumerate(data[:display_count], 1):
                # æ ‡é¢˜ï¼šä¼˜å…ˆä½¿ç”¨ text å­—æ®µå‰ 50 å­—
                title_text = item.get('text', item.get('title', 'æ— æ ‡é¢˜'))[:50]
                if len(item.get('text', '')) > 50:
                    title_text += '...'
                
                f.write(f"### {i}. {title_text}\n\n")
                
                # å†…å®¹å­—æ®µ
                if 'text' in item:
                    f.write(f"**å†…å®¹**:\n```\n{item['text']}\n```\n\n")
                
                # å…¶ä»–å­—æ®µè¡¨æ ¼
                other_fields = {k: v for k, v in item.items() 
                               if not k.startswith('_') and k != 'text' and v}
                if other_fields:
                    f.write("| å­—æ®µ | å†…å®¹ |\n|------|------|\n")
                    for key, value in list(other_fields.items())[:10]:  # æœ€å¤šæ˜¾ç¤º10ä¸ªå­—æ®µ
                        value_str = str(value)[:100]  # æˆªæ–­é•¿å†…å®¹
                        if len(str(value)) > 100:
                            value_str += '...'
                        f.write(f"| {key} | {value_str} |\n")
                    f.write("\n")
                
                f.write("---\n\n")
            
            if len(data) > 100:
                f.write(f"\n> å…± {len(data)} æ¡æ•°æ®ï¼Œæ­¤å¤„ä»…å±•ç¤ºå‰ 100 æ¡ã€‚å®Œæ•´æ•°æ®è¯·æŸ¥çœ‹ JSON æ–‡ä»¶ã€‚\n")


# ============ é¢„è®¾é…ç½® ============

class Presets:
    """å¸¸ç”¨ç½‘ç«™é¢„è®¾é…ç½®"""
    
    @staticmethod
    def twitter(username: str, download_media: bool = False) -> ExtractorConfig:
        """Twitter/X æ¨æ–‡æŠ“å–"""
        
        def extract_full_text(element_html: str) -> str:
            """æå–å®Œæ•´æ¨æ–‡æ–‡æœ¬ï¼Œå¤„ç†å±•å¼€åçš„é•¿æ–‡æœ¬"""
            return element_html.strip()
        
        return ExtractorConfig(
            name=f"Twitter @{username}",
            url_pattern=rf"x\.com/{username}",
            item_selector='article[data-testid="tweet"]',
            field_selectors={
                'id': 'a[href*="/status/"]',
                'text': '[data-testid="tweetText"]',
                'time': 'time',
                'author': 'div[data-testid="User-Name"] a',
                'likes': '[data-testid="like"]',
                'replies': '[data-testid="reply"]',
                'retweets': '[data-testid="retweet"]'
            },
            scroll_times=50,
            scroll_delay=2.5,
            expand_selectors=[
                '[data-testid="tweet-text-show-more-link"]',
            ],
            expand_delay=1.5,
            download_media=download_media,
            field_processors={
                'id': lambda x: re.search(r'/status/(\d+)', str(x)).group(1) if re.search(r'/status/(\d+)', str(x)) else x,
                'likes': lambda x: int(re.search(r'(\d+)', str(x).replace(',', '')).group(1)) if re.search(r'(\d+)', str(x)) else 0,
                'replies': lambda x: int(re.search(r'(\d+)', str(x).replace(',', '')).group(1)) if re.search(r'(\d+)', str(x)) else 0,
                'retweets': lambda x: int(re.search(r'(\d+)', str(x).replace(',', '')).group(1)) if re.search(r'(\d+)', str(x)) else 0,
            },
            sort_field='time'
        )
    
    @staticmethod
    def zhihu_answers() -> ExtractorConfig:
        """çŸ¥ä¹å›ç­”æŠ“å–"""
        return ExtractorConfig(
            name="çŸ¥ä¹å›ç­”",
            url_pattern=r"zhihu\.com/question/\d+",
            item_selector='.AnswerCard, .ContentItem.AnswerItem',
            field_selectors={
                'author': '.AuthorInfo-name',
                'content': '.RichContent-inner',
                'votes': '.VoteButton--up',
                'comments': '.ContentItem-action:has(.CommentIcon)'
            },
            scroll_times=30,
            expand_selectors=['.ContentItem-more', '.RichContent-inner--collapsed']
        )
    
    @staticmethod
    def douban_reviews() -> ExtractorConfig:
        """è±†ç“£å½±è¯„/ä¹¦è¯„æŠ“å–"""
        return ExtractorConfig(
            name="è±†ç“£è¯„è®º",
            url_pattern=r"douban\.com/subject/\d+/reviews",
            item_selector='.review-item',
            field_selectors={
                'title': '.main-bd h2 a',
                'author': '.main-hd .name',
                'rating': '.main-title-rating',
                'content': '.short-content',
                'votes': '.action-btn.up span'
            },
            scroll_times=20
        )
    
    @staticmethod
    def github_issues() -> ExtractorConfig:
        """GitHub Issues æŠ“å–"""
        return ExtractorConfig(
            name="GitHub Issues",
            url_pattern=r"github\.com/[^/]+/[^/]+/issues",
            item_selector='[data-testid="issue-row"]',
            field_selectors={
                'title': 'a[data-testid="issue-title"]',
                'number': 'span[title]',
                'status': '[data-testid="issue-row-status"]',
                'author': '[data-testid="issue-row-author"]'
            },
            scroll_enabled=False  # GitHub ç”¨åˆ†é¡µï¼Œä¸ç”¨æ»šåŠ¨
        )


# ============ ä½¿ç”¨ç¤ºä¾‹ ============

def main():
    """æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨æ¡†æ¶"""
    import sys
    
    # æ£€æŸ¥å‚æ•°
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•:")
        print(f"  python3 {sys.argv[0]} <preset> [options]")
        print("")
        print("å¯ç”¨é¢„è®¾:")
        print("  twitter <username>  - æŠ“å– Twitter æ¨æ–‡")
        print("  zhihu               - æŠ“å–çŸ¥ä¹å›ç­”")
        print("  douban              - æŠ“å–è±†ç“£è¯„è®º")
        print("  github              - æŠ“å– GitHub Issues")
        print("")
        print("ç¤ºä¾‹:")
        print(f"  python3 {sys.argv[0]} twitter elonmusk")
        return
    
    preset = sys.argv[1]
    spider = CDPSpider()
    
    # æ ¹æ®é¢„è®¾åˆ›å»ºé…ç½®
    if preset == 'twitter':
        username = sys.argv[2] if len(sys.argv) > 2 else input("è¾“å…¥ Twitter ç”¨æˆ·å: ")
        config = Presets.twitter(username)
    elif preset == 'zhihu':
        config = Presets.zhihu_answers()
    elif preset == 'douban':
        config = Presets.douban_reviews()
    elif preset == 'github':
        config = Presets.github_issues()
    else:
        print(f"âŒ æœªçŸ¥é¢„è®¾: {preset}")
        return
    
    # æ‰§è¡ŒæŠ“å–
    data = spider.crawl(config)
    
    if data:
        spider.save(data, preset, config)
        print(f"\nğŸ‰ å®Œæˆ! å…±æŠ“å– {len(data)} æ¡æ•°æ®")
    else:
        print("\nâŒ æŠ“å–å¤±è´¥")


if __name__ == '__main__':
    main()
